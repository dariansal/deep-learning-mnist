{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h3><b><u> Multilayer Perceptron with MNIST Dataset</u></b></h1></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### __Importing Libraries, Classes, and Functions__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn #provides classes/modules for making neural networks\n",
    "import torch.nn.functional as F #a module with common nn functions (operations on tensors/high dim matrices; activations)\n",
    "import torch.optim as optim #contains SGD (Stochastic gradient descent)\n",
    "from torchvision import datasets, transforms #includes MNIST\n",
    "from torch.utils.data import DataLoader #allows shuffling and minbatches\n",
    "from torch.nn import Linear\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### __The Model Blueprint__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a NN class that encapsulates all components of NN, instantiate later\n",
    "# with __call__ object name can be treated as function that calls certain function in class (predefine)\n",
    "#inherits from nn.Module class; many classes for different layers\n",
    "class SimpleNN(nn.Module): \n",
    "    def __init__(self):\n",
    "        super(SimpleNN, self).__init__() #explicitly call parent class constructor to initialize stuff\n",
    "                                    #parent technically initialized too and tied to child but you only access child\n",
    "        self.fc1 = Linear(28**2, 128) #first fully connected layer has 784 input neurons (performs matrix mult to calculate z)\n",
    "                                 #next layer has 16 neurons\n",
    "        self.fc2 = Linear(128, 64)\n",
    "        self.fc3 = Linear(64, 10)\n",
    "    \n",
    "    def forward(self, pixels):\n",
    "        '''MNIST is (batch_size = ..., channels = 1, height = 28, width = 28)\n",
    "        Fully connected layers expect input tensors to be (batch_size, num_features); labels already liked this\n",
    "        is actually multiplied by a vector (to get the weighted sum)\n",
    "        Size of first dimension of tensor is batch size, -1 infers dim of the vector/features (784)\n",
    "        Reshapes mini-batch into 2D tensor whre each row is image and column is flattened version\n",
    "        Often need to flatten into fewer dimensions'''\n",
    "        pixels = pixels.view(-1, 28**2) #dimension of input vector\n",
    "        \n",
    "        pixels = F.relu(self.fc1(pixels))  # Apply ReLU activation\n",
    "        pixels = F.relu(self.fc2(pixels))  # Apply ReLU activation\n",
    "        pixels = self.fc3(pixels)  # Output layer (logits)\n",
    "        return pixels\n",
    "    \n",
    "#Instantiate model\n",
    "model = SimpleNN()\n",
    "\n",
    "#Loss function\n",
    "mse = nn.CrossEntropyLoss() #making instance of this class to use the functions in it\n",
    "\n",
    "#Other optimizers than SGD like Adam, takes momentum into account for adaptive learning rate\n",
    "optimizer = optim.SGD(model.parameters(), lr = 0.05) #iterable (like list) of parameters passed to optimizer\n",
    "\n",
    "#Load MNIST\n",
    "#defines transformation to turn images into tensors then normalize them\n",
    "#normalize transforms the pixel values more condensed/similar so training is faster\n",
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize(0.5, 0.5)])\n",
    "\n",
    "#transforms and downloads data\n",
    "train_dataset = datasets.MNIST(root = './data', train = True, download = True, transform = transform)\n",
    "\n",
    "#creates iterator that provides batches of data during training\n",
    "#the batches are in form of (image, label) tuple which are just tensors/vectors\n",
    "train_loader = DataLoader(train_dataset, batch_size = 64, shuffle = True)\n",
    "\n",
    "#Adds batch size to image tensor in train loader\n",
    "test_dataset = datasets.MNIST(root = './data', train = False, download = True, transform = transform)\n",
    "test_loader = DataLoader(test_dataset, batch_size = 64, shuffle = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### __Training__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss:  0.3972\n",
      "Epoch 2, Loss:  0.0898\n",
      "Epoch 3, Loss:  0.1087\n",
      "Epoch 4, Loss:  0.0269\n",
      "Epoch 5, Loss:  0.0092\n"
     ]
    }
   ],
   "source": [
    "'''Each parameter has its own special tensor (created per layer) \n",
    "with a number for its partial derivative in .grad attribute of tensor; step() accesses these'''\n",
    "\n",
    "#Hyperparameter\n",
    "num_epochs = 5\n",
    "\n",
    "for epoch in range(num_epochs): #will do training cycle 5 times\n",
    "\n",
    "    #each iteration is one minibatch of images/labels\n",
    "    for images, labels in train_loader: \n",
    "        optimizer.zero_grad() #zeroing out gradient ignores .grad and recalculates partials in backprop\n",
    "        outputs = model(images) #passes this to forward\n",
    "\n",
    "        #numclasses is components; ex. 4 = [0,0,0,1,0,0,...]\n",
    "        target = F.one_hot(labels, num_classes=10).float()\n",
    "    \n",
    "        loss = mse(outputs, target)\n",
    "        loss.backward() #backward propogation to compute gradiaent\n",
    "        optimizer.step() #updates model parameters (takes \"step\")\n",
    "    \n",
    "    #single number tensors converted to normal number with .item(); 4 decimal places as floating point number\n",
    "    print(f'Epoch {epoch + 1}, Loss: {loss.item(): .4f}')\n",
    "\n",
    "#Saves these parameters\n",
    "'''Model has state dictionary, a Python dictionary that maps to the weights and biases. Extracts this\n",
    "and stores it in a file with .pth convention by convention to store PyTorch parameters or entire models. Now\n",
    "that it is written to disk, the model does not need to be retrained everytime to get parameters. The matrix\n",
    "contains a weight matrix and bias vector for each layer.'''\n",
    "torch.save(model.state_dict(), 'model.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### __Saving and Loading the Model__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model():\n",
    "    # Instantiate the model\n",
    "    model = SimpleNN()\n",
    "    \n",
    "    #Update model's parameters to what it was after training\n",
    "    model.load_state_dict(torch.load('model.pth'))\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    return model\n",
    "\n",
    "model = load_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### __Evalulating/Testing the Model__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy:  96.67\n"
     ]
    }
   ],
   "source": [
    "'''Sometimes in training some neurons turned off (called dropout) to prevent overfitting, but when testing \n",
    "you want all neurons in model to be used, so eval fixes this as well as other settings for testing so model\n",
    "acts \"normally\"'''\n",
    "model.eval()\n",
    "\n",
    "#Counters\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "'''with - disable gradient discent for the for loop and enable when for loop ends\n",
    "Don't store unnecessary numbers (eg. intermediate activations) for calculating gradient; \n",
    "Saves memory and its faster\n",
    "'''\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        outputs = model(images)\n",
    "        \n",
    "        #Return two tensors for max values (brightest neurons) and their indices\n",
    "        '''\n",
    "        Functions in Python can return multiple values by returning tuple which is extracted\n",
    "        outputs.data dimensions are (batch_size, num_classes)\n",
    "        Each row of outputs.data is list of outputs from each output neuron (1 per class) per training example\n",
    "        For SGD outputs.data will 2D array, where all arrays/rows make mini-batch\n",
    "        max has 2D tuple (2 outputs): the max values in each row and the indices of the outputs\n",
    "        We don't care about the output number, just the index/number it predicted, so convention is to store \n",
    "        unimportant stuff in _\n",
    "        Second output returns index of highest activation, the predicted class (same as number in this case)\n",
    "        '''\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "\n",
    "        #Update Counters\n",
    "        total += labels.size(0) #size of first dimension (batch size)\n",
    "\n",
    "        #Compare each each tensor, which returns new tensor with each componenet being True or False\n",
    "        #Sum counts the amount of True in the tensor\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print(f'Test Accuracy: {(correct/total) * 100: .2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### __Trying a New Image__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted class: 4\n"
     ]
    }
   ],
   "source": [
    "transform = transforms.Compose([transforms.ToTensor(),transforms.Normalize((0.5,), (0.5,))])\n",
    "\n",
    "#Preprocess using functions of the Image class\n",
    "def preprocess_image(image_path):\n",
    "    image = Image.open(image_path).convert('L')  #Convert to grayscale\n",
    "    image = image.resize((28, 28))  # Resize to 28x28\n",
    "    image = transform(image)  #Make it a tensor then normalize\n",
    "    image = image.unsqueeze(0)  #Add another dimension (batch_size) to image; not used, but specific shape necessary\n",
    "    return image\n",
    "\n",
    "# Load and preprocess the image \n",
    "image_path = \"4.png\" #\\\\ means \\\n",
    "image = preprocess_image(image_path)\n",
    "\n",
    "#Get model out of training mode into evalution mode\n",
    "model.eval()\n",
    "\n",
    "# Disable gradient computation for speedup\n",
    "with torch.no_grad():\n",
    "    output = model(image)\n",
    "\n",
    "# Get the predicted class\n",
    "_, predicted = torch.max(output.data, 1)\n",
    "print(f'Predicted class: {predicted.item()}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
