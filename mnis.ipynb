{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h3><b><u> Multilayer Perceptron with MNIST Dataset</u></b></h1></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### __Importing Libraries, Classes, and Functions__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn #provides classes/modules for making neural networks\n",
    "import torch.nn.functional as F #a module with common nn functions (operations on tensors/high dim matrices; activations)\n",
    "import torch.optim as optim #contains SGD (Stochastic gradient descent)\n",
    "from torchvision import datasets, transforms #includes MNIST\n",
    "from torch.utils.data import DataLoader #allows shuffling and minbatches\n",
    "from torch.nn import Linear"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### __The Model Blueprint__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a NN class that encapsulates all components of NN, instantiate later\n",
    "# with __call__ object name can be treated as function that calls certain function in class (predefine)\n",
    "#inherits from nn.Module class; many classes for different layers\n",
    "class SimpleNN(nn.Module): \n",
    "    def __init__(self):\n",
    "        super(SimpleNN, self).__init__() #explicitly call parent class constructor to initialize stuff\n",
    "                                    #parent technically initialized too and tied to child but you only access child\n",
    "        self.fc1 = Linear(28**2, 16) #first fully connected layer has 784 input neurons (performs matrix mult to calculate z)\n",
    "                                 #next layer has 16 neurons\n",
    "        self.fc2 = Linear(16, 16)\n",
    "        self.fc3 = Linear(16, 10)\n",
    "    \n",
    "    def forward(self, pixels):\n",
    "        pixels = pixels.view(-1, 28**2) #dimension of input vector\n",
    "        pixels = torch.sigmoid(self.fc1(pixels)) #this includes all z stuff and matrix mult\n",
    "        pixels = torch.sigmoid(self.fc2(pixels))\n",
    "        pixels = torch.sigmoid(self.fc3(pixels))\n",
    "        return pixels\n",
    "    \n",
    "#Instantiate model\n",
    "model = SimpleNN()\n",
    "\n",
    "#Loss function\n",
    "mse = nn.MSELoss() #making instance of this class to use the functions in it\n",
    "\n",
    "#Other optimizers than SGD like Adam, takes momentum into account for adaptive learning rate\n",
    "optimizer = optim.SGD(model.parameters(), lr = 0.001) #iterable (like list) of parameters passed to optimizer\n",
    "\n",
    "#Load MNIST\n",
    "#defines transformation to turn images into tensors then normalize them\n",
    "#normalize transforms the pixel values more condensed/similar so training is faster\n",
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0), 1)])\n",
    "\n",
    "#transforms and downloads data\n",
    "train_dataset = datasets.MNIST(root = './data', train = True, download = True, transform = transform)\n",
    "\n",
    "#creates iterator that provides batches of data during training\n",
    "#the batches are in form of (image, label) tuple which are just tensors/vectors\n",
    "train_loader = DataLoader(train_dataset, batch_size = 64, shuffle = True)\n",
    "\n",
    "#Adds batch size to image tensor in train loader\n",
    "test_dataset = datasets.MNIST(root = './data', train = False, download = True, transform = transform)\n",
    "test_loader = DataLoader(test_dataset, batch_size = 64, shuffle = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### __Training__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss:  0.1737\n",
      "Epoch 2, Loss:  0.1639\n",
      "Epoch 3, Loss:  0.1544\n",
      "Epoch 4, Loss:  0.1489\n",
      "Epoch 5, Loss:  0.1424\n"
     ]
    }
   ],
   "source": [
    "'''Each parameter has its own special tensor (created per layer) \n",
    "with a number for its partial derivative in .grad attribute of tensor; step() accesses these'''\n",
    "\n",
    "#Hyperparameter\n",
    "num_epochs = 5\n",
    "\n",
    "for epoch in range(num_epochs): #will do training cycle 5 times\n",
    "\n",
    "    #each iteration is one minibatch of images/labels\n",
    "    for images, labels in train_loader: \n",
    "        optimizer.zero_grad() #zeroing out gradient ignores .grad and recalculates partials in backprop\n",
    "        outputs = model(images) #passes this to forward\n",
    "\n",
    "        '''MNIST is (batch_size = ..., channels = 1, height = 28, width = 28)\n",
    "        Fully connected layers expect input tensors to be (batch_size, num_features); labels already liked this\n",
    "        is actually multiplied by a vector (to get the weighted sum)\n",
    "        Size of first dimension of tensor is batch size, -1 infers dim of the vector/features (784)\n",
    "        Reshapes mini-batch into 2D tensor whre each row is image and column is flattened version\n",
    "        Often need to flatten into fewer dimensions'''\n",
    "        images.view(images.size(0), -1) \n",
    "\n",
    "        #numclasses is components; ex. 4 = [0,0,0,1,0,0,...]\n",
    "        target = F.one_hot(labels, num_classes=10).float()\n",
    "    \n",
    "        loss = mse(outputs, target)\n",
    "        loss.backward() #backward propogation to compute gradiaent\n",
    "        optimizer.step() #updates model parameters (takes \"step\")\n",
    "    \n",
    "    #single number tensors converted to normal number with .item(); 4 decimal places as floating point number\n",
    "    print(f'Epoch {epoch + 1}, Loss: {loss.item(): .4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### __Evalulating/Testing the Model__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "expected ':' (2414284180.py, line 5)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[47], line 5\u001b[0;36m\u001b[0m\n\u001b[0;31m    with torch.no\u001b[0m\n\u001b[0m                 ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m expected ':'\n"
     ]
    }
   ],
   "source": [
    "'''Sometimes in training some neurons turned off (called dropout_ to prevent overfitting, but when testing \n",
    "you want all neurons in model to be used, so eval fixes this as well as other settings for testing so model\n",
    "acts \"normally\"'''\n",
    "model.eval()\n",
    "\n",
    "#Counters\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "#with: disable gradient discent for the for loop and enable when for loop ends\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images = images.view(images.size(0), 1 * 28 * 28)\n",
    "        outputs = model(images)\n",
    "        \n",
    "        #Return two tensors for max values (brightest neurons) and their indices\n",
    "        '''\n",
    "        Functions in Python can return multiple values by returning tuple which is extracted\n",
    "        outputs.data dimensions are (batch_size, num_classes)\n",
    "        Each row of outputs.data is list of outputs from each output neuron (1 per class) per training example\n",
    "        For SGD outputs.data will 2D array, where all arrays/rows make mini-batch\n",
    "        max has 2D tuple (2 outputs): the max values in each row and the indices of the outputs\n",
    "        We don't care about the output number, just the index/number it predicted, so convention is to store \n",
    "        unimportant stuff in _\n",
    "        Second output returns index of highest activation, the predicted class (same as number in this case)\n",
    "        '''\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "\n",
    "        #Update Counters\n",
    "        total += labels.size(0) #size of first dimension (batch size)\n",
    "\n",
    "        #Compare each each tensor, which returns new tensor with each componenet being True or False\n",
    "        #Sum counts the amount of True in the tensor\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "        print(f'Test Accuracy: {(correct/total) * 100: .2f}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
