{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h3><b><u> Multilayer Perceptron with MNIST Dataset</u></b></h1></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### __Importing Libraries, Classes, and Functions__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'google.colab'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[143], line 11\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mPIL\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Image\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgoogle\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcolab\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m files\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'google.colab'"
     ]
    }
   ],
   "source": [
    "import operator #itemgetter\n",
    "import torch\n",
    "import torch.nn as nn #provides classes/modules for making neural networks\n",
    "from torch.nn import Linear\n",
    "import torch.nn.functional as F #a module with common nn functions (operations on tensors/high dim matrices; activations)\n",
    "import torch.optim as optim #contains optimization algorithms like SGD\n",
    "from torchvision import datasets, transforms #includes MNIST, transform images -> tensors\n",
    "from torch.utils.data import DataLoader, TensorDataset, Subset, random_split #allows shuffling and minbatches\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import shutil\n",
    "from google.colab import files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### __The Model Blueprint__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Can use GPU, which has lots of cores, instead if available. It is good for parallel processing for tasks\n",
    "like matrix multiplication where job can be split up to calculate each element of new matrix. GPU's also have\n",
    "high bandwidth so they can indirectly have faster time talking to memory. This computer doesn't have cuda so\n",
    "run on CoLab.\n",
    "'''\n",
    "#Create a NN class that encapsulates all components of NN, instantiate later\n",
    "# with __call__ object name can be treated as function that calls certain function in class (predefine)\n",
    "#inherits from nn.Module class; many classes for different layers\n",
    "class NumberNN(nn.Module): \n",
    "    def __init__(self):\n",
    "        super(NumberNN, self).__init__() #explicitly call parent class constructor to initialize stuff\n",
    "                                    #parent technically initialized too and tied to child but you only access child\n",
    "        self.fc1 = Linear(in_features = 28**2, out_features = 128) #first fully connected layer has 784 input neurons\n",
    "        self.fc2 = Linear(128, 64)\n",
    "        self.fc3 = Linear(64, 10)\n",
    "    \n",
    "    def forward(self, pixels):\n",
    "        '''MNIST is (batch_size = ..., channels = 1, height = 28, width = 28)\n",
    "        Fully connected layers expect input tensors to be (batch_size, num_features); labels already liked this\n",
    "        Size of first dimension of tensor is batch size, -1 infers dim of the vector/features (784)\n",
    "        '''\n",
    "        pixels = pixels.view(-1, 28**2)  # Flatten the input tensor (batch_size, 1, 28, 28) to (batch_size, 784)\n",
    "        \n",
    "        pixels = F.leaky_relu(self.fc1(pixels), negative_slope= 0.02)\n",
    "        pixels = F.leaky_relu(self.fc2(pixels), negative_slope= 0.02) \n",
    "        pixels = self.fc3(pixels) # Output layer (logits)\n",
    "        return pixels\n",
    "    \n",
    "#Instantiate model\n",
    "model = NumberNN()\n",
    "\n",
    "#Loss function - making instance of this class to use the functions in it\n",
    "cross_entropy = nn.CrossEntropyLoss()\n",
    "\n",
    "#Other optimizers than SGD like Adam, takes momentum into account for adaptive learning rate: .05 good for SGD\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001, betas=(0.9, 0.999), eps=1e-08)\n",
    "\n",
    "'''transformation function object: applies transformations in list sequentially; images into tensors -> normalize\n",
    "normalize transforms the pixel values more condensed/similar so training is faster\n",
    "Normalizing for single channel (good practice to include tuple instead of scalar even if only one channel)'''\n",
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
    "\n",
    "#Load MNIST\n",
    "train_dataset = datasets.MNIST(root = './data', train = True, download = True, transform = transform)\n",
    "\n",
    "#creates iterator that provides batches of data during training\n",
    "#the batches are in form of (image, label) tuple which are just tensors/vectors\n",
    "train_loader = DataLoader(train_dataset, batch_size = 64, shuffle = True)\n",
    "\n",
    "#Adds batch size to image tensor in train loader\n",
    "test_dataset = datasets.MNIST(root = './data', train = False, download = True, transform = transform)\n",
    "test_loader = DataLoader(test_dataset, batch_size = 64, shuffle = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### __Training the Model__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss:  0.2767\n",
      "Epoch 2, Loss:  0.1557\n",
      "Epoch 3, Loss:  0.1065\n",
      "Epoch 4, Loss:  0.2222\n",
      "Epoch 5, Loss:  0.0083\n"
     ]
    }
   ],
   "source": [
    "'''Each parameter has its own special tensor (created per layer) \n",
    "with a number for its partial derivative in .grad attribute of tensor; step() accesses these'''\n",
    "\n",
    "def train_model(model, train_loader, optimizer, loss, num_epochs):\n",
    "    #Hyperparameter\n",
    "    num_epochs = 5\n",
    "\n",
    "    for epoch in range(num_epochs): #will do training cycle 5 times\n",
    "\n",
    "        #each iteration is one minibatch of images/labels\n",
    "        for images, labels in train_loader: \n",
    "            optimizer.zero_grad() #zeroing out gradient ignores .grad and recalculates partials in backprop\n",
    "            outputs = model(images) #passes this to forward\n",
    "\n",
    "            #numclasses is components; ex. 4 = [0,0,0,1,0,0,...]\n",
    "            target = F.one_hot(labels, num_classes=10).float()\n",
    "\n",
    "            loss = cross_entropy(outputs, target)\n",
    "            loss.backward() #backward propogation to compute gradiaent\n",
    "            optimizer.step() #updates model parameters (takes \"step\")\n",
    "\n",
    "        #single number tensors converted to normal number with .item(); 4 decimal places as floating point number\n",
    "        print(f'Epoch {epoch + 1}, Loss: {loss.item(): .4f}')\n",
    "\n",
    "train_model(model, train_loader, optimizer, loss = cross_entropy, num_epochs = 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### __Saving the Model__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Saves these parameters in Colab or locally\n",
    "'''Saves the parameters in colab. Model has state dictionary, a Python dictionary that maps to the weights and \n",
    "biases. Extracts this and stores it in a file with .pth convention by convention to store PyTorch parameters \n",
    "or entire models.'''\n",
    "torch.save(model.state_dict(), 'bestmodel.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Download locally\n",
    "files.download('bestmodel.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Move to correct folder if Colab used\n",
    "model_downloads_path = os.path.join(os.path.expanduser(\"~\"), \"Downloads\", \"bestmodel.pth\")\n",
    "\n",
    "model_destination_path = \"/mnt/c/Users/daria/OneDrive/Practice/nnPractice/bestmodel.pth\"\n",
    "\n",
    "shutil.move(model_downloads_path, model_destination_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### __Loading the Saved Model__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model():\n",
    "    # Instantiate the model\n",
    "    model = NumberNN()\n",
    "    \n",
    "    #Update model's parameters to what it was after training\n",
    "    model.load_state_dict(torch.load('bestmodel.pth'))\n",
    "    \n",
    "    '''Sometimes in training some neurons turned off (called dropout) to prevent overfitting, but when testing \n",
    "you want all neurons in model to be used, so eval fixes this as well as other settings for testing so model\n",
    "acts \"normally\"'''\n",
    "    model.eval()\n",
    "    \n",
    "    return model\n",
    "\n",
    "model = load_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### __Evalulating/Testing the Model__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 96.7\n"
     ]
    }
   ],
   "source": [
    "'''with - disable gradient discent for the for loop and enable when for loop ends\n",
    "Don't store unnecessary numbers (eg. intermediate activations) for calculating gradient; \n",
    "Saves memory and its faster\n",
    "'''\n",
    "\n",
    "def test_model(model, test_loader):\n",
    "    model.eval()\n",
    "\n",
    "    #Counters\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            outputs = model(images)\n",
    "\n",
    "            #Return two tensors for max values (brightest neurons) and their indices\n",
    "            '''\n",
    "            outputs dimensions: (batch_size, num_classes). Num_classes has value of each neuron in output\n",
    "            layer. You have 1 array per training example, and for each mini-batch you have 2D array\n",
    "            max has 2 outputs): the max values in each row and the indices of the max values\n",
    "            We only care about index/number it predicted, so convention to store unimportant stuff in _\n",
    "            1 as parameter input means find max along dimension 1\n",
    "            '''\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "\n",
    "            #Update Counters\n",
    "            total += labels.size(0) #size of first dimension (batch size)\n",
    "\n",
    "            #Compare each tensor, which returns new tensor with each component being True or False\n",
    "            #Sum counts the amount of True in the tensor\n",
    "            correct += (predicted == labels).sum().item()\n",
    "        return (correct/total) * 100\n",
    "\n",
    "accuracy = test_model(model, test_loader)\n",
    "print(f'Test Accuracy: {accuracy}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### __Inputting a New Image__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probability of 6:  99.91%\n",
      "Probability of 4:  0.05%\n",
      "Probability of 8:  0.03%\n",
      "Probability of 5:  0.01%\n",
      "Probability of 0:  0.00%\n",
      "Probability of 2:  0.00%\n",
      "Probability of 9:  0.00%\n",
      "Probability of 7:  0.00%\n",
      "Probability of 3:  0.00%\n",
      "Probability of 1:  0.00%\n",
      "\n",
      "Predicted number: 6\n"
     ]
    }
   ],
   "source": [
    "def pixelate_image(image, pixelation_level):\n",
    "    \n",
    "    '''/ is floor division. Tuples have indices. Resize is changing # of pixels after knowing new \n",
    "    size of image, what makes it appear pixelated is then enlarging image so the few pixels are larger.\n",
    "    Bilinear interpoloation is to get value of each pixel, look at weighted average of 4 nearest pixels \n",
    "    of where it would go to determine new value. Weighted based on what is closest to it. Then split those \n",
    "    big pixels up again so the size is same as original but looks more pixelated.'''\n",
    "\n",
    "    new_size = (image.size[0] // pixelation_level, image.size[1] // pixelation_level)\n",
    "    pixelated = image.resize(new_size,resample=Image.BILINEAR)\n",
    "\n",
    "    pixelated = pixelated.resize((28, 28))\n",
    "    \n",
    "    return pixelated\n",
    "\n",
    "#Preprocess using functions of the Image class\n",
    "def preprocess_image(image_path):\n",
    "    image = pixelate_image(image_path, 8)\n",
    "    image = transform(image)  #Make it a tensor then normalize\n",
    "    image = image.unsqueeze(0)  #Add another dimension (batch_size) to image; not used, but specific shape necessary\n",
    "    return image\n",
    "\n",
    "# Load and preprocess the image \n",
    "image = Image.open(\"6.1.png\").convert('L') #\"instantiating\", gives size as attribute in tuple\n",
    "\n",
    "pixelated_image_visual = pixelate_image(image, 5)\n",
    "\n",
    "image_transformed = preprocess_image(image)\n",
    "\n",
    "\n",
    "# Disable gradient computation for speedup\n",
    "with torch.no_grad():\n",
    "    output = model(image_transformed)\n",
    "    _, predicted = torch.max(output, dim = 1)\n",
    "    probabilities = F.softmax(output, dim = 1) #probabilities[i] that contains softmax for outputs of i+1 image \n",
    "\n",
    "#(number, probability tensor)\n",
    "prob_tuples = list(enumerate(probabilities[0])) \n",
    "\n",
    "#to sort by max, extract probability with itemgetter\n",
    "prob_tuples.sort(key = itemgetter(1), reverse = True)\n",
    "\n",
    "for number, probability in prob_tuples:\n",
    "    print(f'Probability of {number}: {probability.item() * 100: .2f}%')\n",
    "\n",
    "print(f'\\nPredicted number: {predicted.item()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### __Visualizing the Transformed Image__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAgMAAAD9CAYAAAAh3HQXAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjkuMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy80BEi2AAAACXBIWXMAAA9hAAAPYQGoP6dpAAAhnElEQVR4nO3daXRUZdb28asqAwECCQQZBExAEHUhRBAHXDyAUwiCEkZBZIZuWxm0bUyraGQUWyaDQ9sNAVFRUFogQYIoQwNLNMyoIIggUyCEKYEMVKXeD7bP42vrqQx1TlXl/H9r9addnH1rS9VVd91nH4fH4/EIAADYltPfCwAAAP5FGAAAwOYIAwAA2BxhAAAAmyMMAABgc4QBAABsjjAAAIDNEQYAALC50NK+0OFwmLkOAKXAjDAAZmBnAAAAmyMMAABgc4QBAABsrtRnBgCgPDhvBPift/NG7AwAAGBzhAEAAGyOMAAAgM0RBgAAsDnCAAAANkcYAADA5ggDAADYHGEAAACbIwwAAGBzhAEAAGyOMAAAgM0RBgAAsDnCAAAANkcYAADA5ggDAADYHGEAAACbIwwAAGBzhAEAAGyOMAAAgM0RBgAAsDnCAAAANkcYAADA5ggDAADYXKi/FwAA8K+aNWsa1ouKiipUR+BjZwAAAJsjDAAAYHOEAQAAbI4wAACAzXGAsBKKjIzUjTfeqG7dusnhcPx/tZycHGVkZOiHH35QSUmJn1YIAAgkDo/H4ynVC3/1oYLAExkZqe7du+uJJ55Q69atFR4e/l+v8Xg8ysvL04oVKzRnzhzt3LlTLpfLD6tFeZTyr2tA4b0j8HE3QeXn7b2DMFBJNG3aVIsWLdJtt92mkJCQUv2ZvLw8/eMf/9D06dN1+vRpk1cIXyAMwAyEgcqPMFDJOZ1ODRw4UC+88IKaNm1a5j/v8Xi0e/du9e3bV999950JK4QvEQZQHklJSYb1GTNmGNZPnjxpWO/Xr59h/dixY4Z1mM/bewcHCIOYw+HQI488otdee61cQeDna7Ru3VqLFy9WTEyMj1cIAAgGhIEg1r59e82dO1eRkZEVvlZ8fLz++te/yunkPwkAsBve+YPUtddeqzlz5vgkCEg//dwwYsQIdezY0SfXAwAED8JAELr22mv14Ycfqm3btj69blRUlMaMGcPuAADYDO/6QaZp06ZasmSJ4uPjTbl+hw4ddPXVV5tybQBAYCIMBJHw8HA9/fTTatOmjWk9atWqpYSEBNOuDwAIPISBIFGlShVNnjxZw4cPN7WP0+n0es8xAKByYRxxkBgwYICefPLJUg8UAmAf3g7+LliwwLDu7QtAkyZNDOs33XSTYZ05A4GPnYEg0KlTJ02dOpUgAAAwBWEgwHXu3FmLFy9W/fr1/b0UAEAlRRgIYLVr19Yrr7xieRDgwUUAYC+EgQBVu3ZtLViwwNQ7B35LXl6eMjMzLe0JAPAvwkAAcjqdmjFjhrp372557w0bNujw4cOW9wUA+A9hIMA4HA6NHDlSffv29Uv/t956S8XFxX7pDQDwD8JAgLnxxhv14osvqlq1apb3Tk9P16ZNmyzvCwDwL+YMBJDrrrtO77//vurVq2d57xMnTmj06NE6d+6c5b0BGLv22msN6wsXLjSsV3SQ2KeffmpY37x5c4WuD/9jZyBAREREaMKECWrZsqXlvbOzszVw4EDOCgCATREGAkRycrIGDBhged+cnBwNHDhQ69ats7w3ACAwEAYCQGJioh577DHLHx3sdrv1xhtv6LPPPrO0LwAgsBAG/Kxly5ZauHCh6tSpY2lft9utWbNmaerUqZb2BQAEHg4Q+lHdunU1Y8YMXXXVVZb2dbvdmj17tp599lluIwQAsDPgLyEhIRo3bpzuu+8+y3tnZWXpxRdfJAgAACQRBvwiJCREjz/+uJ588knLe+/atUsPP/yw8vLyLO8NAAhMDo/H4ynVCx0Os9diG23bttWnn36qWrVqWdp37969SkpK0sGDBy3tC98p5V/XgMJ7h3fe5gCkp6cb1jt06FCh/jt27DCsd+3a1bCenZ1dof4wn7f3DnYGLNawYUO9++67lgeBy5cva/LkyQQBAMB/IQxYqHr16kpJSVGLFi0s7Zufn6/HH39cS5YssbQvACA4EAYsNHnyZA0bNszSnh6PR8nJyVq4cGFQbjEDAMzHrYUWcDgceuCBBzRkyBBLBwt5PB4tW7ZMixYtUklJiWV9AQDBhZ0BC3Tr1k3z589XdHS0pX03bdqkkSNH6uLFi5b2BQAEF8KAyRo0aKCpU6eqdu3alvY9duyYkpOTeQohAMArwoCJ6tWrp/fee8/yJxG6XC5NmjRJW7ZssbQvACA4EQZM8vNgoU6dOlna1+Vy6aWXXlJaWpqlfQEAwYuhQyZwOp166qmnNHHiRFWpUsXS3uvXr1fXrl1VUFBgaV9YIxjvCOG946cvB0a8hfdHHnmkQv1Pnz5tWL/77rsN63v37q1Qf/gfQ4f8oE2bNho/frzlQeDbb7/VkCFDCAIAgDIhDPhY69at9f777ysmJsbSvnl5eZo4caKOHDliaV8AQPAjDPhQZGSknn/+eV177bWW9nW5XBo9erQ++OADS/sCACoHwoCPREREaO7cuUpKSrK0r8fj0ZIlS7R06dKg/D0ZAOB/hAEfSUpK0kMPPWT5Yal9+/ZpzJgxunz5sqV9AQCVB2HAB6655hpNmTLF8gODJ0+e1KBBg5Sbm2tpXwBA5UIYqKCwsDBNmTJFTZo0sbTvlStXNGfOHGVlZVnaFwBQ+fCgogp66KGH1K9fP0t7ejweTZ48WTNmzLC0L4DyGTVqlGG9onMEioqKKtSfOQJgZ6ACmjZtqkmTJiksLMzSvps3b9brr78ul8tlaV8AQOVEGCiniIgITZ48WbGxsZb23bhxox566CGdOXPG0r4AgMqLMFBOffr0sfw2wvPnz2vs2LE6fvy4pX0BAJUbYaAc4uLilJKSooiICMt6njt3TiNGjNDOnTst6wkAsAfCQBmFh4dr4sSJatq0qWU9PR6P/v73v2vZsmWW9QQA2AdhoIx69eql/v37W9bP4/Fo4cKFmjRpEhMGAQCmIAyUQVxcnKZPn67QUOvuyNy/f7+Sk5OZMAgAMA1zBkopOjpaM2bMUOPGjS3r+f3336t///46deqUZT0BlM11113n9TWTJ082dQ3PPvusYX358uWm9kfwY2eglIYMGaKePXta1q+oqEgpKSkcGAQAmI4wUAp16tTRo48+alm/4uJiPffcc1q8eLFlPQEA9kUY8CIkJERPP/20mjVrZlnPtWvXKjU1VW6327KeAAD7Igx40bJlS40cOVJOpzX/qnbv3q3hw4d7nTUOAICvEAYMOJ1OjRs3TlFRUZb0y8/P1zPPPKPs7GxL+gEAIBEGDMXHx1t6aDAjI0OZmZmW9QMAQCIM/K7q1avr9ddfV82aNS3pd+TIEaWkpPAkQgCA5Zgz8Du6d++udu3aWdKruLhY06dP1759+yzpB8B3JkyY4PU1tWvXrlCP9PR0w/qcOXMqdH2AnYHfEBkZqSeeeMKyQ4OZmZlKS0uzpBcAAL9GGPgVh8OhESNGqE2bNpb0O3XqlJ566ikVFhZa0g8AgF8jDPxKbGyskpOTLXn+gNvt1nPPPafvvvvO9F4AAPwewsAvOBwOjRo1SvXq1bOk37/+9S+98847lvQCAOD3EAZ+oUmTJhoyZIglvY4ePapnn32WnwcAAH5HGPgPh8OhP/zhD2rQoIHpvYqLizV58mR+HgAABATCwH/ExsZq0KBBlvTasmWLFi1aZEkvAAC8Yc7Af9x5552WnBVwuVyaNWuWCgoKTO8FoOLi4+MN6717965wj4sXLxrWk5OTDev+HlYWEhJiWK9bt65hvXr16ob1K1euGNZPnjxpWC8uLjasg50BST/NFRg9erQcDofpvdLS0rR69WrT+wAAUFqEAUkJCQlq27at6X1yc3M1c+ZMUioAIKDYPgw4HA498MADlswVWLx4MYcGAQABx/ZhoEWLFkpKSjK9T25urubOnauSkhLTewEAUBa2DwOJiYmKjIw0tYfb7dZrr72mAwcOmNoHAIDysPXdBHXr1tWjjz5q+sHBb775RrNmzaq0uwJVq1ZVnTp1yvznrly5ouzsbBNWBAAoC1uHgXvvvVfNmjUztYfH49HcuXN1/vx5U/uYLSQkRGFhYWrUqJGaNm2qm2++WbfccoskqV69el5vv/otly5d0pYtW5Sbm6tly5bp4sWL2rFjh9xuN4csAcBCtg0D4eHhGjRokOm7Anv37tWSJUtM7WEGp9OpRo0aqVq1aurevbtatWqlW2+9VbVq1dJVV13lkx41atRQz549JUkjR45UUVGRfvzxR+Xk5Gj16tU6deqUVq9eraNHj8rj8fikJ1BWAwYMMKxHRERUuMd7771nWP/6668rdH1vf2c7d+5sWL/vvvsM661btzasx8XFGda9/Tt0u92G9UOHDhnWJ0yYYFjPyMgwrNuBbcNA586d1alTJ1N7uN1upaamBvyuQFhYmMLCwtS0aVO1aNFCvXv3VnR0tG677TZFRESoatWqlqyjSpUqat68uZo3b6727durpKRE58+f17JlyzRnzhx9++23Xt8UAABlZ8sw4HQ6lZSUpPDwcFP7fPPNN1q6dKmpPcojNDRUcXFxuummm9SmTRvddNNNatWqlWrVqqXo6Gh/L+9/OZ1O1a5dWyNGjFCfPn2Umpqq2bNnKzc3199LA4BKxZZhoHr16kpISDC9z+effx4QuwIOh0ORkZG66667NHz4cNWsWVNt27ZVRESEJfMVfCEqKkrPPPOMEhIS1LdvXx0+fNjfSwKASiM4Pgl8rEWLFoqJiTG1x9mzZ/Xmm2+a2sOb6OhotWrVSr169VLXrl0VGxursLAwv66pIpxOp9q1a6clS5YQCADAh2wZBq6//nrVqFHD1B47duzweqjFDFFRUbrjjjt0//3366677tJ1110XNN/+S6tdu3b68MMPNWDAAB04cIDDhQBQQZXrU6KUfPGUMSMul0tz5syx7Pa4qKgotW7dWr1791ZiYqIaN26sKlWqWNLbX9q2bauNGzfqqaee0rvvvksgAIAKsF0YuOaaa9S+fXtTexw9elRffvmlqT2qVq2q2NhYdenSRUOHDlXLli3ldNproGS9evX0xhtvyOFw6J133iEQAEA5OTylfAe14vG+Vrj++uu1c+dOU785L1q0SIMGDTLl2k6nU02aNNGQIUM0evRoRUVFmdInmOTk5KhDhw7av3+/v5diumAMPIH+3uHtZ7QtW7YY1tu1a+e1h7dbYu+8807Dek5OjmF9/PjxhvWuXbsa1hs3bmxYD3bnzp0zrP88QM2IP3729SVv7x222xlITEw09ZbC/Px8zZ0716fXDA0NVcOGDZWQkKBevXqpTZs2iomJCfg3WatcddVVGjdunMaOHcvkQgAoB1uFAafTqRtuuMHUD9H9+/dr7969Fb6Ow+H43wl9PXr00P/8z/8oOjqaAPA7hg8froyMDKWnp/t7KQAQdGwVBqpWrap77rnHtOuXlJQoNTVVly9fLvc16tSpo1tuuUVdu3ZVly5d1KxZMwJAKYSFhWnWrFnatGlTQMx2AIBgYqswcNNNN/lsrv5vuXLlirZu3VquP1u/fn0NGjRII0eO1DXXXGP6dMTKqHHjxrr99tu1evVqfy8FAIKKrcJAQkKCIiMjTbv+zp079cMPP5Tpz0RERKhbt256+eWX1aRJE5NWZg9VqlRR165dCQMAUEa2CQMOh0O33XabqT3279+voqKiUr++WrVqmjFjhoYNG8ZOgI/cc889ioqK0oULF/y9FAAIGra5Mb1x48amh4E1a9aU+rUOh0MvvfSSRo0aRRDwoeuuu07x8fH+XgYABBXb7AyEhob65Lnjvyc3N7dM5wW6dOmiQYMG2W5QkNlCQkLUu3dvbdiwwd9LQZCoU6eOYd0XP98dO3bMsL57927D+ttvv21YN3uqqjcFBQWG9aNHjxrWT506ZVhv1qyZYb1BgwaG9Vq1ahnWS/NFMdjnDHhjm0+i6OhoUz94c3JyvP4H/bP69esrJSWFgUEmSUxMVM2aNf29DAAIGrYJA4mJiabuDKxdu1Z5eXmlem2PHj106623mrYWu2vcuLHXiW4AgP9jmzBg5q5ASUmJ15Glv1xHnz59TFsLpPDwcNWuXdvfywCAoGGbMGDmrkBJSYm+/fbbUr22WbNmatu2rWlrAQCgrGwRBqpUqaLu3bubdv0ffvjB6wGhn1WrVs3UWQcAAJSVLe4mcDgcpu4M5OTkKDc317TrBwOXy6UzZ86U6ql6NWrUIBABQACxRRiIjo5W1apV/b2MSsPtduvy5cvatWuXzp49q2XLlun8+fP697//Xaow0Lx5c/Xv31/jxo0zf7EAAK9sEQZat26tRo0amXb9S5cumXbtQHDu3Dnl5+dr69atOnz4sDZs2KDvvvtOBw8eVElJSZmv9+WXX6p58+YmrBQoO2+7VNWqVatwj7NnzxrWvd2n7+1MktvtNqx7m8jpbS7H8uXLDeveZqwcOXLEsO5yuQzr3tbnbc6AN5X9Pbw0bBEGzPbRRx+V6huxJOXn5+vChQsBedrd4/GouLhY+fn52r59u3JycvTxxx9rz549On78uAoKCrz+pS0Nh8Ohvn37+mDFAABfIAz4QHFxcalf+/3332v//v264447TFxR6RUUFOjkyZPasGGDvvnmG6Wnp6ugoMBrkq8I5gAAQGAhDFjM4/Fo9erVfg0DLpdL27dv15o1a/TJJ59o586dKioq8rrV6CtdunRRdHS0adcvKioq9TRIAABhwC/mzZunYcOGKTY21tK+R44c0eeff66MjAytXr3aL7+TValSRX/84x8VEhJiWo/c3FxlZWWZdn0AqGwIA35w/Phxvfnmm5o8ebKpH4oul0vZ2dlav369li5dqi1btujs2bPlOvTnK0lJSWrRooWpPVwuV6nPcAAACAN+M3v2bHXp0kUdO3b0+bUvXLigPXv2aO7cuVq/fn3AbJnHxcXp+eef98npbCOZmZm6ePGiqT0AoDIhDPhJYWGhRowYoaVLlyo+Pr7C17t48aK++OILrVq1SmvXrtX+/ft9cvLfV8LDw/XMM8/ohhtuMLWP2+3WsmXL2BkAgDIgDPjRwYMH1adPH82bN0/t27dXaGjZ/u8oKSnR/v37lZ6errfeektHjx5VUVGRSautmMTERA0dOtT0PocPH9a2bdtM74PKw9s9/oWFhYb10ux01a9f37AeExNjWE9JSTGsr1ixwrCek5NjWDfz7iHpp9uJjfztb38zrFf0wPXRo0cN66V90FxlRhjwgYqMOj548KASEhLUpUsXzZgxQ40aNVJ4ePjvvr6wsFBHjhxRZmamli5dqp07d+rSpUsB/U24YcOGmj17dpnDTnls3LhRZ86cMb0PAFQmhAEf6N27t958881yfyAXFhbq448/1oYNG9S+fXt17dpVbdq0+a/XbdmyRZ988omysrJ0/vz5Cq7aGiEhIXrppZcsuXPC5XJp5cqVAR2MACAQ2SIMFBcXy+12m3ZyPzY2VnXq1PG6FefNuXPnlJGRoYyMjN/cVgu2Dzmn06lBgwapV69eXrcJfSErK0tr1qwxvQ8AVDa2eITxV199pcOHD5t2/bi4OLVt29an1/R4PP/1v2DicDg0dOhQvfrqq5Y9JCozM5MZ4wBQDrYIAy6Xy9R760NCQtSzZ09Lvv0GA6fTqSFDhujVV1+17FHFP//UAgAoO1uEgZKSEp04ccLUHgkJCapZs6apPYLBL3cEzJ4n8Etr167V3r17LesHAJWJLcJAcXGxPvvsM1N71KpVy/aP5Y2MjNTIkSMt3RGQfro1bObMmQE1VwEAgoktDhBKPz0Pu6ioSFWqVDHl+jVq1NDYsWM1ePBgv4779ZdatWpp3rx56t69uyW3EP7S2rVruU8Y5ZadnW1Y37Vrl2G9c+fOXns0aNDAsD5x4kTD+rhx4wzr/n4WR+vWrQ3rL7zwgmE9KSnJl8v5L88995xhnduRbbIzIEm7d+/WgQMHTO3Rs2dP9e7d29QegahWrVqaP3++kpKSLA8ChYWFmjlzZsAOWwKAYGCbMHD+/Hl9/vnnpvaoVq2aJkyYoLi4OFP7BJK6desqLS1NPXr08Ev/2bNna+PGjX7pDQCVhW3CgCTTzw1IUsuWLTV+/HjDKYKVQXh4uBITE7V582Y9+OCDfllDZmamZs6cacufZQDAl2wVBrZt26aTJ0+a3mf48OFKSEgwvY+/xMXFacaMGfr444/VrFkzv6zh9OnTGj9+fIUHPQEAbBYGTp06pU2bNpneJzw8XDNnzlS9evVM72Ulp9OpPn36aPny5Xr88cf9tvvhdrs1depU7d692y/9AaCysVUYcLlcysjIkNvtNr1Xs2bNtGjRIt1+++2m9zJbaGioOnXqpPXr12vBggVq1aqV39bidruVmpqqN998029rAIDKxlZhQJLWrVuns2fPWtLr3nvv1fLly3XPPfdY0s8M9erV08SJE7Vy5Up16NDB0kFCvyUrK0sTJkzg7gEA8CGHp5RD7yvTqN2PP/7Y0kNvp0+fVkpKitLS0rw+Gz1QVK1aVXfddZdefPFFnz93obwuXryojh07aufOnf5eit8E2zMqpOB/70hMTDSsr1y50us1KvqQtHXr1hnW3377bcN6bm6uYd3bU0Xvvvtuw7q3LzwVHULm7ZCwtzkC06ZNq1D/ysDbe4ftdgYk6YMPPrC0X926dZWamqqpU6cqIiLC0t5l1aBBA40fP16rVq3SRx99FDBBoLCwUOPHj+ecAACYwDYTCH9pzZo1Wr9+vTp16mRZz5CQEI0ZM0YxMTGaNGmSDh48aFlvb0JDQ9W2bVt169ZNjzzyiNdvCf6QkZGhtLQ0biMEABPYMgzk5ubq5Zdf1p133qmwsDDL+oaEhGjQoEGKj49Xv379dPDgQb/O069Zs6buuOMODR48WN27d7f0eQJlsXLlSo0aNUrFxcX+XgoAVEq2/JlA+mkA0fz58/3Su1WrVlq/fr2WLVumu+++W7Vr17a0f8OGDTV48GCtX79eq1atUv/+/QM2CKxYsUJDhgyx7NAnANiRLXcGpJ+eZPjz7kDLli0t71+vXj11795dCQkJOnTokN544w0tXrxYZ8+e9emtj+Hh4QoNDdX111+vhg0bqlOnTho4cKBiYmIqfKjJTB6PR+np6Ro6dChBAABMZtswIEmHDh3SlClTNH/+fFWtWtUvawgPD9f111+vOXPm6C9/+YvWrFmjN954Q99++60uXbpU5uvVqVNHMTExatOmjW688UbdcsstatGiherUqaMaNWqY8E9gjhUrVmjYsGEEAQCwgK3DgCQtWbJE999/vx5++GG/3wLVqFEjDRs2TAMHDtT27dv11ltvKT8/v1R/Nj4+Xu3atVNsbKxiY2MVGhoa0N/8f09+fr7ee+89JScn69y5c/5eDgDYgi3nDPza1VdfrU2bNqlJkyb+XoqtFRQUaMyYMZo/fz53DfwO5gwEnpSUFK+veeGFF8xfSBA7ceKEYT05OdmwvmjRIl8up1JizkApnDhxQuPGjfPryX67y8/P19ixYzVv3jyCAABYjDDwH6tXr/bb3QV2V1BQoCeffFLz5s0Lym++ABDsCAP/UVxcrGnTpmnv3r3+Xoqt5Ofna8yYMfrnP//JjgAA+Alh4BcOHz6sKVOmlPrQHipm9+7deuihhzR//nx2BADAjwgDv/LBBx9o+vTpfDiZyO126/3331evXr2UkZHBjgAA+Blh4Fc8Ho9SU1O1dOlSfy+lUjp58qRGjBihIUOGBNTzGQDAzmw/Z+C3XLhwQSNHjpTL5VJSUpLfBhJVNunp6XriiScIAQAQYJgzYCA8PFz9+vXT66+/HrCz+4NBXl6epk2bprfeesvrc9VhLBh/vrLje8evDR482LD+7LPPGtabN2/uy+X43NGjRw3r3nZaU1NTDeuHDx8u65LwK97eO9gZMFBcXKx33nlHHo9H06dP19VXX+3vJQUVt9ut7du3609/+pO2b9/O2QAACFCcGfDC4/Ho3XffVceOHZWVleXv5QSNM2fOaNq0abr77ruVlZVFEACAAEYYKAWPx6ODBw+qT58+2rBhA5MKvThw4IAGDhyoCRMmKC8vz9/LAQB4QRgog8OHDysxMVEvvfQSgeA3XLx4UWlpaerQoYMyMzP9vRwAQClxZqCMCgoKNHHiRHk8Hj322GOqXbu2v5fkdxcuXNCePXs0btw47dy5U263299LAgCUATsD5XDlyhWlpKTowQcf1LFjx/y9HL8oKSlRbm6u0tLS1LFjR3Xu3Fnbtm0jCABAECIMlFNJSYk2bdqkpKQkpaenB+UtX+Xx8/mJP//5z2rdurVGjhypXbt28bMJAAQxfiaooKysLD388MPq1auXRo8erVatWikkJMTfy/K5goICrVu3TqmpqcrKytKZM2f8vSQAgI8wdMiHoqKi1LNnTw0bNkzx8fGVYlBRdna23nnnHa1atUpbtmxRUVGRv5dka8G4A8V7h3dRUVGG9XvvvdewfvPNNxvWQ0ONv/d5C/d79uwxrG/bts2wnpOTY1iH+Rg6ZKELFy4oLS1Nb7/9tm655RYNGDBADzzwgOLi4vy9tDJxuVzKysrS6tWrtWDBAh05csTfSwIAmIgwYAK3262tW7dq69atmj59ugYPHqykpCTdfPPNXhO6v3g8HuXl5emrr77SvHnztHLlSh7lDAA2EZifTJXIiRMnNG3aNM2aNUv33nuvHnzwQfXo0UMxMTH+Xpqys7NVWFiozMxMff311/rkk0906NAhpgUCgM0QBixSWFiolStXKiMjQ6+88oo6d+6sRx99VC1atFB4eLhpfT0ej4qKipSXl6cdO3YoOztbK1eulNvt1hdffKH8/Hzl5+cH5W/RAADfIAxYrKSkRPv27dO+ffu0ePFitWzZUj169FC3bt0UFhamatWqqX79+uW+/pUrV3T8+HGdO3dOn3zyic6cOaNVq1apoKBAP/74ow//SQAAlQVhwI/Onz+vTZs2afPmzXr++eclSTExMYqPjy/3NS9fvqytW7fK5XKpsLDQRysFAFRmhIEA4PF4dPnyZUk/fZh7ezY4AAC+xJwBIIgE49kO3jsA//P23sE4YgAAbI4wAACAzREGAACwOcIAAAA2RxgAAMDmCAMAANgcYQAAAJsjDAAAYHOEAQAAbI4wAACAzREGAACwOcIAAAA2RxgAAMDmCAMAANgcYQAAAJsjDAAAYHOEAQAAbI4wAACAzREGAACwOcIAAAA2RxgAAMDmCAMAANgcYQAAAJsjDAAAYHOEAQAAbI4wAACAzREGAACwOcIAAAA2RxgAAMDmCAMAANgcYQAAAJsjDAAAYHOEAQAAbI4wAACAzREGAACwOcIAAAA2RxgAAMDmCAMAANgcYQAAAJsL9fcCAFRuHo/H30sA4AU7AwAA2BxhAAAAmyMMAABgc6U+M8DvfgAAVE7sDAAAYHOEAQAAbI4wAACAzREGAACwOcIAAAA2RxgAAMDmCAMAANgcYQAAAJsjDAAAYHP/D8iuZxQANSe6AAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.subplot(1,2,1) #create subplot first\n",
    "plt.imshow(image, cmap='gray')\n",
    "plt.axis('off')\n",
    "\n",
    "#dimensions need to be correct\n",
    "plt.subplot(1,2,2)\n",
    "plt.imshow(pixelated_image_visual, cmap='gray')\n",
    "plt.axis('off') \n",
    "\n",
    "plt.show() #not needed for Jupyter"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
