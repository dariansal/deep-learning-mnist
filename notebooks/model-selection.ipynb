{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h2><b><u> Multilayer Perceptron with MNIST Dataset</u></b></h2></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __Importing Libraries, Classes, and Functions__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "import torch\n",
    "import torch.nn as nn #provides classes/modules for making neural networks\n",
    "import numpy as np\n",
    "from torch.nn import Linear\n",
    "import torch.nn.functional as F #a module with common nn functions (operations on tensors/high dim matrices; activations)\n",
    "import torch.optim as optim #contains optimization algorithms like SGD\n",
    "from torchvision import datasets, transforms #includes MNIST, transform images -> tensors\n",
    "from torch.utils.data import DataLoader, TensorDataset, Subset, ConcatDataset #allows shuffling and minbatches\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import KFold, ParameterGrid\n",
    "import os\n",
    "import shutil\n",
    "import random\n",
    "import json\n",
    "#!pip install torch-xla\n",
    "#import torch_xla\n",
    "#import torch_xla.core.xla_model as xm #tpu\n",
    "#from google.colab import files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __Initializing the Hyperparameters__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Hypers:\n",
    "    def __init__(self, hyperpath):\n",
    "        with open(hyperpath, 'r') as file:\n",
    "            hyper_dict = json.load(file)\n",
    "        self.l_rate = hyper_dict['learning_rate']\n",
    "        self.b_size = hyper_dict['batch_size']\n",
    "        self.epochs = hyper_dict['epochs']\n",
    "        self.drop_rate = hyper_dict['dropout_rate']\n",
    "        self.hidden_one = hyper_dict['hidden_one']\n",
    "        self.hidden_two = hyper_dict['hidden_two']\n",
    "        self.n_slope = hyper_dict['n_slope']\n",
    "        self.augment = hyper_dict['augment']\n",
    "        self.prob_augment = hyper_dict['prob_augment']\n",
    "\n",
    "#can feed this into functions that need hypers\n",
    "hypers = Hypers('../config/hyperparameters.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __The Model Blueprint__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NumberNN(nn.Module): \n",
    "    def __init__(self, hidden_one, hidden_two, drop_prob, n_slope):\n",
    "        super(NumberNN, self).__init__() #explicitly call parent class constructor to initialize stuff\n",
    "                                    #parent (nn.Module) technically initialized too and tied to child but you only access child\n",
    "        self.fc1 = Linear(in_features = 28**2, out_features = hidden_one) #num features = num neurons in input layer (1 neuron per feature/vector component)\n",
    "        self.fc2 = Linear(hidden_one, hidden_two)\n",
    "        self.fc3 = Linear(hidden_two, 10)\n",
    "        self.n_slope = n_slope\n",
    "\n",
    "        self.dropout = nn.Dropout(p=drop_prob) \n",
    "    \n",
    "    def forward(self, x):\n",
    "        '''MNIST is (batch_size = ..., channels = 1, height = 28, width = 28), a 3D tensor. To plot you want 2D 28x28,\n",
    "        and for input layer you need to make the pixel reprsentation 1D (batch_size, 784). Batch size since it processes\n",
    "        a whole batch at once. -1 infers dim of the vector/features (784)\n",
    "        '''\n",
    "        x = x.view(-1, 28**2)  # Flatten the input tensor (batch_size, 1, 28, 28) to (batch_size, 784)\n",
    "\n",
    "        z1 = self.fc1(x) #Weight matrix + bias vector\n",
    "        a1 = F.leaky_relu(z1, negative_slope= self.n_slope) \n",
    "        a1 = self.dropout(a1) #Starting dropout in layer 2; don't drop inputs\n",
    "\n",
    "        z2 = self.fc2(a1)\n",
    "        a2 = F.leaky_relu(z2, negative_slope= self.n_slope) \n",
    "        a2 = self.dropout(a2)\n",
    "\n",
    "        outputs = self.fc3(a2) #logits - softmax(logit) = p_class\n",
    "        \n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __Data Loading and Preparation__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### __Choosing the Device__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [],
   "source": [
    "#device = xm.xla_device() #Sets up commmunication between CPU and TPU\n",
    "#device = torch.device(\"mps\")\n",
    "device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### __Defining the Transformations__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Transformation function object applies transformations in list sequentially (e.g. images into tensors \n",
    "-> normalize). Normalizing/condensing pixels to 0-1 to make model focuses more on relationships instead\n",
    "of brightness levels.'''\n",
    "def transformations(maybe_augment):\n",
    "    transform_norm = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
    "\n",
    "\n",
    "    #degrees - max rotation; translation - max horizontal/vert shift, scale - scaling range, shear - up to 10 degrees for both axes\n",
    "    #all these are hypers\n",
    "    aug = transforms.RandomAffine(degrees = 15, translate = (0.03, 0.002), scale = (0.9, 1.05), shear = (-7, 7))\n",
    "\n",
    "    if (maybe_augment == True):\n",
    "        #.2 -> ranges from .8 to 1.2 of orig (1)\n",
    "        transform_aug = transforms.Compose([aug, transforms.ColorJitter(brightness=.22, contrast=0.25), \n",
    "                                        transforms.ToTensor(),\n",
    "                                        transforms.Normalize((0.5,), (0.5,))\n",
    "                                        ])\n",
    "    else:\n",
    "        transform_aug = transform_norm\n",
    "    \n",
    "    return transform_norm, transform_aug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_norm, transform_aug = transformations(hypers.augment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### __Loading the Data__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [],
   "source": [
    "#like list of tuples of (image (tensor), label)...\n",
    "train_dataset = datasets.MNIST(root = '../data', train = True, download = False, transform = None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __Data Augmentation__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### __Viewing the Current Distribution of Classes__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1: 6742\n",
      "7: 6265\n",
      "3: 6131\n",
      "2: 5958\n",
      "9: 5949\n",
      "0: 5923\n",
      "6: 5918\n",
      "8: 5851\n",
      "4: 5842\n",
      "5: 5421\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' With Augmentation\\nTest Accuracy for 0: 99.49%\\nTest Accuracy for 1: 99.12%\\nTest Accuracy for 8: 98.87%\\nTest Accuracy for 7: 98.83%\\nTest Accuracy for 4: 98.57%\\nTest Accuracy for 3: 98.12%\\nTest Accuracy for 5: 98.09%\\nTest Accuracy for 6: 97.91%\\nTest Accuracy for 2: 97.87%\\nTest Accuracy for 9: 96.53%\\n    '"
      ]
     },
     "execution_count": 289,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def distribution(dataset):\n",
    "    freq = {}\n",
    "    for label in range(0, 10):\n",
    "        freq[label] = 0\n",
    "\n",
    "    for image, label in dataset:\n",
    "        freq[label] += 1\n",
    "\n",
    "    #list of tuples\n",
    "    freq_sorted = sorted(freq.items(), key = itemgetter(1), reverse = True)\n",
    "    for number, freq in freq_sorted:\n",
    "        print(f'{number}: {freq}')\n",
    "\n",
    "distribution(train_dataset)\n",
    "\n",
    "'''\n",
    "Test Accuracy for 1: 99.82%\n",
    "Test Accuracy for 6: 98.96%\n",
    "Test Accuracy for 0: 98.88%\n",
    "Test Accuracy for 9: 98.02%\n",
    "Test Accuracy for 8: 97.84%\n",
    "Test Accuracy for 3: 97.82%\n",
    "Test Accuracy for 4: 97.76%\n",
    "Test Accuracy for 7: 97.67%\n",
    "Test Accuracy for 2: 97.09%\n",
    "Test Accuracy for 5: 97.09%\n",
    "'''\n",
    "''' With Augmentation\n",
    "Test Accuracy for 0: 99.49%\n",
    "Test Accuracy for 1: 99.12%\n",
    "Test Accuracy for 8: 98.87%\n",
    "Test Accuracy for 7: 98.83%\n",
    "Test Accuracy for 4: 98.57%\n",
    "Test Accuracy for 3: 98.12%\n",
    "Test Accuracy for 5: 98.09%\n",
    "Test Accuracy for 6: 97.91%\n",
    "Test Accuracy for 2: 97.87%\n",
    "Test Accuracy for 9: 96.53%\n",
    "    '''\n",
    "#5 is fewest, can apply tilting/brightness"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### __Visualizing the Augmentation__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [],
   "source": [
    "def side_by_side_plot(image1, image2, title1, title2):\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.imshow(image1, cmap=\"grey\")\n",
    "    plt.title(f'{title1}')\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "    plt.subplot(1,2,2)\n",
    "    plt.title(f'{title2}')\n",
    "    plt.imshow(image2, cmap = \"grey\")\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "def get_image(dataset, index):\n",
    "    image, __ = dataset[index]\n",
    "    return image.squeeze(0) #remove singleton batch_size dimension (provides no new info); now 2D (28x28) array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#transformation applied on the fly when image accessed; every epoch sees dif versions of images\n",
    "norm_dataset = datasets.MNIST(root = '../data', train = True, download = False, transform = transform_norm) #true on Colab\n",
    "aug_dataset = datasets.MNIST(root = '../data', train = True, download = False, transform = transform_aug)\n",
    "\n",
    "index = 4\n",
    "norm_image = get_image(norm_dataset, index)\n",
    "aug_image = get_image(aug_dataset, index)\n",
    "\n",
    "side_by_side_plot(norm_image, aug_image, \"Original\", \"Augmented\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### __Getting Proper Proportions of Augmented and Original Data__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Get data -> split into subsets -> wrap them in DataSetT class and give them transformation attribute, \n",
    "define methods necessary for DataLoader-> merge them back together'''\n",
    "class DataSetT:\n",
    "    def __init__(self, images, labels, transform):\n",
    "        self.images = images\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self): #subset has length\n",
    "        return len(self.images)\n",
    "    \n",
    "    def __getitem__(self, index): #works with DataLoader, lets me do dataset[index]; SubSet has this implemented but not with transform\n",
    "        sample = self.images[index]\n",
    "        sample_transformed = self.transform(sample) # __call__ method of Compose on Data instance: for sammple (s) in self.transforms, s = t(s)\n",
    "        label = self.labels[index] \n",
    "        return sample_transformed, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract(dataset):\n",
    "    images = []\n",
    "    labels = []\n",
    "    for image, label in dataset:\n",
    "        images.append(image)\n",
    "        labels.append(label)\n",
    "    return images, labels\n",
    "\n",
    "def calculate_indices(dataset, hypers):\n",
    "    len_train = len(dataset)\n",
    "\n",
    "    num_norm_indices = int(len_train * (1 - hypers.prob_augment))\n",
    "    num_aug_indices = int(len_train * hypers.prob_augment)\n",
    "\n",
    "    all_indices = np.random.permutation(len_train) #random array of numbers 0 to n-1\n",
    "\n",
    "    #part of permutation is indices for norm images, rest is augmented\n",
    "    norm_indices = all_indices[0:num_norm_indices] \n",
    "    aug_indices = all_indices[num_norm_indices:]\n",
    "\n",
    "    return norm_indices, aug_indices\n",
    "\n",
    "def create_dataset_t(dataset, indices):\n",
    "    subset = Subset(dataset, indices)\n",
    "    images, labels = extract(subset)\n",
    "    return DataSetT(images, labels, transform_norm)\n",
    "\n",
    "\n",
    "def norm_aug_trainloader(dataset, hypers):\n",
    "    norm_indices, aug_indices = calculate_indices(dataset, hypers)\n",
    "\n",
    "    #For norm and aug, create a subset of the data, create Data instance so relevant transformation is applied when DataLoader gets image\n",
    "    norm_dataset = create_dataset_t(dataset, norm_indices)\n",
    "    aug_dataset = create_dataset_t(dataset, aug_indices)\n",
    "\n",
    "   #Concat determines what dataset contains that index -> recalculates index for that dataset -> delegates __getitem__ call to that Data instance\n",
    "    norm_aug_trainset = ConcatDataset([norm_dataset, aug_dataset])\n",
    "    return DataLoader(norm_aug_trainset, hypers.b_size, shuffle = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __Model Evaluation and Selection__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### __Alter Hyper for Grid Search__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [],
   "source": [
    "def alter_hyper(hypers, hypercombo):\n",
    "    hypers.l_rate = hypercombo['l_rate']\n",
    "    hypers.b_size = hypercombo['b_size']\n",
    "    hypers.epochs = hypercombo['epochs']\n",
    "    hypers.drop_rate = hypercombo['drop_rate']\n",
    "    hypers.hidden_one = hypercombo['hidden_one']\n",
    "    hypers.hidden_two = hypercombo['hidden_two']\n",
    "    hypers.n_slope = hypercombo['n_slope']\n",
    "    hypers.augment = hypercombo['augment']\n",
    "    hypers.prob_augment = hypercombo['prob_augment']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### __K-folds Cross Validation__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [],
   "source": [
    "#already ensure validation only used once per sample\n",
    "def k_folds(train_dataset, hypers, hypercombo = None, grid_search = False):\n",
    "\n",
    "    kf = KFold(n_splits = 10, shuffle = True, random_state = 20)\n",
    "    validation_accuracies = []\n",
    "\n",
    "    if grid_search:\n",
    "        alter_hyper(hypers, hypercombo)\n",
    "\n",
    "    '''by default split only works with indices arrays (use np.arange(len(data))) unless it can use length attribute of dataset, which Pytorch provides.\n",
    "    This works too: norm_dataset.__len__(), dunder methods simplify syntax'''\n",
    "    for fold, (train_indices, val_indices) in enumerate(kf.split(train_dataset)):\n",
    "        if (fold == 0):\n",
    "            print(f'Fold {fold + 1}')\n",
    "        else:\n",
    "            print(f'\\nFold {fold + 1}')\n",
    "\n",
    "        #My actual validation\n",
    "        val_transformed = create_dataset_t(train_dataset, val_indices)\n",
    "        val_loader = DataLoader(val_transformed, batch_size = 512, shuffle=False)\n",
    "\n",
    "        train_subset = Subset(train_dataset, train_indices)\n",
    "        train_loader = norm_aug_trainloader(train_subset, hypers)\n",
    "        \n",
    "        model = NumberNN(hypers.hidden_one, hypers.hidden_two, hypers.drop_rate, hypers.n_slope).to(device) #ensures parameters reset and each fold is independent\n",
    "        cross_entropy = nn.CrossEntropyLoss()\n",
    "        adam = optim.Adam(model.parameters(), lr= hypers.l_rate, betas=(0.9, 0.999), eps=1e-08) #forget info from EWMA\n",
    "\n",
    "        last_epoch_val_accuracy = train_model(model, train_loader, optimizer = adam, loss_f = cross_entropy, \n",
    "                                              num_epochs = hypers.epochs, val_loader = val_loader)\n",
    "\n",
    "        validation_accuracies.append(last_epoch_val_accuracy)\n",
    "\n",
    "    mean_val_accuracy = np.mean(validation_accuracies)\n",
    "    #Use average validation accuracy across all folds to evaluate model\n",
    "    return mean_val_accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_val_accuracy = k_folds(train_dataset, hypers)\n",
    "print(f'\\n\\033[1mMean Validation Accuracy Across All Folds: {mean_val_accuracy:.2f}%\\033[0m\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### __Hyperparameter Tuning with Gridsearch__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "#All other hyperparameters previously tested\n",
    "hyperdict = {\n",
    "    'l_rate': [hypers.l_rate],\n",
    "    'b_size': [hypers.b_size],\n",
    "    'epochs': [hypers.epochs],\n",
    "    'drop_rate': [hypers.drop_rate],\n",
    "    'hidden_one': [hypers.hidden_one],\n",
    "    'hidden_two': [hypers.hidden_two],\n",
    "    'n_slope': [hypers.n_slope],\n",
    "    'augment': [hypers.augment],\n",
    "    'prob_augment': [0.25, hypers.prob_augment]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GridSearch(hyperdict, train_dataset, hypers):\n",
    "    best_hypers = None\n",
    "    best_mean_val_accuracy = 0\n",
    "    orig_hypers = hypers\n",
    "\n",
    "    #iterable - generates on the fly to save memory; generates all possible dictionary combos\n",
    "    for hypercombo in ParameterGrid(hyperdict): \n",
    "        #sometimes you save model also\n",
    "        hypers = orig_hypers #reset hypers\n",
    "        print(hypercombo)\n",
    "\n",
    "        mean_val_accuracy = k_folds(train_dataset, hypers, hypercombo, grid_search=True)\n",
    "        print(f'\\n\\033[1mMean Validation Accuracy Across All Folds: {mean_val_accuracy:.2f}%\\033[0m\\n')\n",
    "        \n",
    "        if (mean_val_accuracy > best_mean_val_accuracy):\n",
    "            best_mean_val_accuracy = mean_val_accuracy\n",
    "            best_hypers = hypercombo\n",
    "    \n",
    "    hypers = orig_hypers\n",
    "    return best_hypers, best_mean_val_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_hypers, best_mean_val_accuracy = GridSearch(hyperdict, train_dataset, hypers)\n",
    "print(f'\\n\\033[1mBest Hyperparameters: {best_hypers}, Best Mean Validation Accuracy:{best_mean_val_accuracy: .2f}%\\033[0m')\n",
    "\n",
    "#Always tune; batch size and learning rate relationship not always linear\n",
    "# Best Hyperparameters: {'l_rate': 0.001}, Mean Validation Accuracy: 97.59% - no augmentation\n",
    "#Same hypers with augmentation - Mean Val: 98%\n",
    "#98.1 with .2\n",
    "#.35 is 97.77"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __Training the Model__ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, optimizer, loss_f, num_epochs, val_loader = None):\n",
    "    model.train() #prevents weird stuff (e.g. optimizer being weird)\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "\n",
    "        #each iteration is one minibatch of images/labels\n",
    "        for images, labels in train_loader: \n",
    "            images, labels = images.to(device), labels.to(device) #moves the tensor from virtual machine memory (mapped to RAM of server) to TPU memory\n",
    "\n",
    "            '''Each parameter has its own special tensor (created per layer) \n",
    "            with a number for its partial derivative in .grad attribute of tensor; step() accesses these. Zeroing\n",
    "            out graident ignores .grad and recalculates partials in backprop.'''\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            outputs = model(images) #passes this to forward\n",
    "\n",
    "            target = F.one_hot(labels, num_classes=10).float() #numclasses is numcomponents; ex. 4 = [0,0,0,1,0,0,...]\n",
    "\n",
    "            loss = loss_f(outputs, target) #tensor with scalar and computation graph\n",
    "            loss.backward() #backward propogation to compute gradient\n",
    "\n",
    "            optimizer.step() #updates model parameters (takes \"step\")\n",
    "\n",
    "            #Colab\n",
    "            #xm.optimizer_step(optimizer) #ensures gradient calculated first\n",
    "            #xm.mark_step()\n",
    "\n",
    "\n",
    "        #Print out stats for first and last epoch for k-folds cross validation\n",
    "        if val_loader and (epoch == 0 or epoch == num_epochs - 1):\n",
    "            train_accuracy = evaluate(model, train_loader)\n",
    "            val_accuracy = evaluate(model, val_loader)\n",
    "\n",
    "            print(f'Epoch {epoch + 1} -- Loss:{loss.item(): .4f}, Training Accuracy:{train_accuracy : .2f}%, Validation Accuracy:{val_accuracy : .2f}%')\n",
    "        \n",
    "            #return final validation accuracy to be averaged in k-folds\n",
    "            if (epoch == num_epochs - 1):\n",
    "                return val_accuracy\n",
    "        elif not val_loader and (epoch == 0 or epoch == num_epochs - 1):\n",
    "            print(f'Epoch {epoch + 1} -- Loss:{loss.item(): .4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_aug_loader = norm_aug_trainloader(train_dataset, hypers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### __Training__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 -- Loss: 0.2507\n",
      "Epoch 20 -- Loss: 0.0246\n"
     ]
    }
   ],
   "source": [
    "model = NumberNN(hypers.hidden_one, hypers.hidden_two, hypers.drop_rate, hypers.n_slope).to(device)\n",
    "cross_entropy = nn.CrossEntropyLoss()\n",
    "adam = optim.Adam(model.parameters(), hypers.l_rate, betas=(0.9, 0.999), eps=1e-08) \n",
    "\n",
    "train_model(model = model, train_loader = norm_aug_loader, optimizer = adam, loss_f = cross_entropy, num_epochs = hypers.epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __Saving the Model__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mlp-lr0.001-b512-ep20-0.2\n"
     ]
    }
   ],
   "source": [
    "trained_model = f'mlp-lr{hypers.l_rate}-b{hypers.b_size}-ep{hypers.epochs}-{hypers.prob_augment}'\n",
    "best_model = 'best-mlp-lr0.001-b512-ep20-0.2p-mapfold-augmented'\n",
    "print(trained_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save these parameters in Colab or locally\n",
    "'''Model has state dictionary, a Python dictionary that maps to the weights and \n",
    "biases. Extracts this and stores it in a file with .pth convention by convention to store PyTorch parameters \n",
    "or entire models.'''\n",
    "torch.save(model.state_dict(), f'../models/{trained_model}.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#If using Colab, download locally\n",
    "files.download('colabmodel.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Then if using Colab, move to correct folder\n",
    "model_downloads_path = os.path.join(os.path.expanduser(\"~\"), \"Downloads\", \"colabmodel.pth\")\n",
    "\n",
    "model_destination_path = \"/Users/darian/Documents/Coding/deep-learning-mnist/models/colabmodel.pth\"\n",
    "\n",
    "shutil.move(model_downloads_path, model_destination_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __Loading the Saved Model__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model():\n",
    "    # Instantiate the model\n",
    "    model = NumberNN(hypers.hidden_one, hypers.hidden_two, hypers.drop_rate, hypers.n_slope).to(device)\n",
    "    #Update model's parameters to what it was after training\n",
    "    #model.load_state_dict(torch.load(f'../models/{trained_model}.pth'))\n",
    "    model.load_state_dict(torch.load(f'../models/{best_model}.pth'))\n",
    "    \n",
    "    '''Sometimes in training some neurons turned off (called dropout) to prevent overfitting, but when testing \n",
    "you want all neurons in model to be used, so eval fixes this as well as other settings for testing so model\n",
    "acts \"normally\"'''\n",
    "    model.eval()\n",
    "\n",
    "    return model\n",
    "\n",
    "model = load_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __Evalulating/Testing the Model__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### __Testing Accuracy for all Testing Data__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, loader):\n",
    "    model.eval()\n",
    "\n",
    "    #Counters\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    '''with - gradient descent will be tracked outside of with block. Not tracking ->\n",
    "    doesn't store unnecessary numbers (eg. intermediate activations) for calculating gradient; \n",
    "    Saves memory and its faster\n",
    "    '''\n",
    "    with torch.no_grad():\n",
    "        for images, labels in loader:\n",
    "            images, labels = images.to(device), labels.to(device) \n",
    "\n",
    "            outputs = model(images) #self.forward(images), uses __call__\n",
    "            '''Return two tensors in tuple for max values (brightest neurons) and their indices per training \n",
    "            example in mini batch. Convention to store unimportant stuff in _. 1 as parameter input means \n",
    "            find max along dimension 1. Outputs = (batch_size, num_classes). Max outputs the max output neuron\n",
    "            and index (number) per batch.\n",
    "            '''\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "\n",
    "            #Update Counters\n",
    "            total += labels.size(0) #size of first dimension (each minibatch)\n",
    "\n",
    "            #Compare each tensor, which returns new tensor with each component being True or False\n",
    "            #Sum counts the amount of True in the tensor\n",
    "            correct += (predicted == labels).sum().item()\n",
    "        return (correct/total) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 98.35%\n"
     ]
    }
   ],
   "source": [
    "test_dataset = datasets.MNIST(root = '../data', train = False, download = False, transform = transform_norm) #True on Colab\n",
    "\n",
    "test_loader = DataLoader(test_dataset, batch_size = 512, shuffle = False)\n",
    "\n",
    "accuracy = evaluate(model, loader = test_loader)\n",
    "print(f'Test Accuracy:{accuracy : .2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### __Testing Accuracy Per Digit__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [],
   "source": [
    "def digit_indices(dataset, digit):\n",
    "    indices = []\n",
    "    for index, (image, label) in enumerate(dataset):\n",
    "        if label == digit:\n",
    "            indices.append(index)\n",
    "    return indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy for 0: 99.49%\n",
      "Test Accuracy for 1: 99.12%\n",
      "Test Accuracy for 8: 98.87%\n",
      "Test Accuracy for 7: 98.83%\n",
      "Test Accuracy for 4: 98.57%\n",
      "Test Accuracy for 3: 98.12%\n",
      "Test Accuracy for 5: 98.09%\n",
      "Test Accuracy for 6: 97.91%\n",
      "Test Accuracy for 2: 97.87%\n",
      "Test Accuracy for 9: 96.53%\n"
     ]
    }
   ],
   "source": [
    "accuracies = {}\n",
    "\n",
    "for i in range(0,10):\n",
    "    one_digit_indices = digit_indices(dataset = test_dataset, digit = i)\n",
    "    one_digit_test_set = Subset(test_dataset, one_digit_indices)\n",
    "    one_digit_loader = DataLoader(one_digit_test_set, batch_size= 512, shuffle = False)\n",
    "\n",
    "    accuracy = evaluate(model, loader = one_digit_loader)\n",
    "    accuracies[i] = accuracy\n",
    "\n",
    "sorted_accuracies = sorted(accuracies.items(), key = itemgetter(1), reverse = True)\n",
    "\n",
    "for digit, accuracy in sorted_accuracies:\n",
    "    print(f'Test Accuracy for {digit}:{accuracy: .2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __Inputting a New Image__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### __Pixelation__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pixelate_image(image, pixelation_level):\n",
    "    \n",
    "    '''/ is floor division. Tuples have indices. Resize is changing # of pixels after knowing new \n",
    "    size of image, what makes it appear pixelated is then enlarging image so the few pixels are larger.\n",
    "    Bilinear interpoloation is to get value of each pixel, look at weighted (based on distance) average of \n",
    "    4 nearest pixels of where it would go to determine new value. Then split those big pixels up again \n",
    "    so the size is same as original but looks more pixelated.'''\n",
    "\n",
    "    #open and greyscale image (compress to 1 channel)\n",
    "\n",
    "    new_size = (image.size[0] // pixelation_level, image.size[1] // pixelation_level)\n",
    "    pixelated = image.resize(new_size,resample=Image.BILINEAR)\n",
    "\n",
    "    pixelated = pixelated.resize((28, 28))\n",
    "    \n",
    "    return pixelated\n",
    "\n",
    "#Preprocess using functions of the Image class\n",
    "def preprocess_image(image, pixelation_level):\n",
    "    image = pixelate_image(image, pixelation_level)\n",
    "    image = transform_norm(image)  #images -> pixel values-> normalize\n",
    "    image = image.unsqueeze(0)  #Add singleton dimension (batch_size) to image; not used, but specific shape necessary for model\n",
    "    return image\n",
    "\n",
    "def preprocess_no_pixelation(image):\n",
    "    image = image.resize((28, 28))\n",
    "    image = transform_norm(image) \n",
    "    image = image.unsqueeze(0)\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probability of 5: 95.337%\n",
      "Probability of 3: 4.663%\n",
      "Probability of 1: 0.000%\n",
      "Probability of 7: 0.000%\n",
      "Probability of 2: 0.000%\n",
      "Probability of 9: 0.000%\n",
      "Probability of 8: 0.000%\n",
      "Probability of 0: 0.000%\n",
      "Probability of 6: 0.000%\n",
      "Probability of 4: 0.000%\n",
      "\n",
      "This is the number 5!\n"
     ]
    }
   ],
   "source": [
    "original = Image.open('../data/test-images/mnist.png').convert('L')\n",
    "\n",
    "#pixelation slightly worse outcome because its different noise than model experienced in training\n",
    "pixelation_level = 1\n",
    "pixelated_image = pixelate_image(original, pixelation_level)\n",
    "\n",
    "image_transformed = preprocess_image(original, pixelation_level).to(device)\n",
    "#image_transformed = preprocess_no_pixelation(image).to(device)\n",
    "with torch.no_grad():\n",
    "    outputs = model(image_transformed)\n",
    "    _, predicted = torch.max(outputs, dim = 1)\n",
    "\n",
    "    #softmax takes care of negative outputs\n",
    "    probabilities = F.softmax(outputs, dim = 1) #probabilities[i] that contains softmax for outputs of i+1 image \n",
    "\n",
    "#[(number, probability tensor)...]; enumerate packs into tuple with counter\n",
    "prob_tuples = list(enumerate(probabilities[0])) \n",
    "\n",
    "#to sort by max, extract probability with itemgetter\n",
    "prob_tuples.sort(key = itemgetter(1), reverse = True)\n",
    "\n",
    "for number, probability in prob_tuples:\n",
    "    print(f'Probability of {number}:{probability.item() * 100: .3f}%')\n",
    "\n",
    "print(f'\\nThis is the number {predicted.item()}!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __Visualizing the Pixelated Image__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "side_by_side_plot(original, pixelated_image, \"Original\", \"Pixelated\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
