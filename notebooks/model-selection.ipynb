{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h2><b><u> Multilayer Perceptron with MNIST Dataset</u></b></h2></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __Importing Libraries, Classes, and Functions__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "import torch\n",
    "import torch.nn as nn #provides classes/modules for making neural networks\n",
    "import numpy as np\n",
    "from torch.nn import Linear\n",
    "import torch.nn.functional as F #a module with common nn functions (operations on tensors/high dim matrices; activations)\n",
    "import torch.optim as optim #contains optimization algorithms like SGD\n",
    "from torchvision import datasets, transforms #includes MNIST, transform images -> tensors\n",
    "from torch.utils.data import DataLoader, TensorDataset, Subset, random_split #allows shuffling and minbatches\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import KFold, ParameterGrid\n",
    "import os\n",
    "import shutil\n",
    "import json\n",
    "#!pip install torch-xla\n",
    "#import torch_xla\n",
    "#import torch_xla.core.xla_model as xm #tpu\n",
    "#from google.colab import files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __Initializing the Hyperparameters__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Hypers:\n",
    "    def __init__(self, hyperpath):\n",
    "        with open(hyperpath, 'r') as file:\n",
    "            hyper_dict = json.load(file)\n",
    "        self.l_rate = hyper_dict['learning_rate']\n",
    "        self.b_size = hyper_dict['batch_size']\n",
    "        self.epochs = hyper_dict['epochs']\n",
    "        self.drop_rate = hyper_dict['dropout_rate']\n",
    "        self.hidden_one = hyper_dict['hidden_one']\n",
    "        self.hidden_two = hyper_dict['hidden_two']\n",
    "        self.n_slope = hyper_dict['n_slope']\n",
    "\n",
    "hypers = Hypers('../config/hyperparameters.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __The Model Blueprint__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with __call__ object name can be treated as function that calls certain function in class (predefine)\n",
    "#inherits from nn.Module class; many classes for different layers\n",
    "class NumberNN(nn.Module): \n",
    "    def __init__(self, hidden_one, hidden_two, drop_prob, n_slope):\n",
    "        super(NumberNN, self).__init__() #explicitly call parent class constructor to initialize stuff\n",
    "                                    #parent (nn.Module) technically initialized too and tied to child but you only access child\n",
    "        self.fc1 = Linear(in_features = 28**2, out_features = hidden_one) #num features = num neurons in input layer (1 neuron per feature/vector component)\n",
    "        self.fc2 = Linear(hidden_one, hidden_two)\n",
    "        self.fc3 = Linear(hidden_two, 10)\n",
    "        self.n_slope = n_slope\n",
    "\n",
    "        self.dropout = nn.Dropout(p=drop_prob) \n",
    "    \n",
    "    def forward(self, x):\n",
    "        '''MNIST is (batch_size = ..., channels = 1, height = 28, width = 28)\n",
    "        Fully connected layers expect input tensors to be (batch_size, num_features); labels already liked this\n",
    "        Size of first dimension of tensor is batch size, -1 infers dim of the vector/features (784)\n",
    "        '''\n",
    "        x = x.view(-1, 28**2)  # Flatten the input tensor (batch_size, 1, 28, 28) to (batch_size, 784)\n",
    "\n",
    "        z1 = self.fc1(x) #Weight matrix + bias vector\n",
    "        a1 = F.leaky_relu(z1, negative_slope= self.n_slope) \n",
    "        a1 = self.dropout(a1) #Starting dropout in layer 2; don't drop inputs\n",
    "\n",
    "        z2 = self.fc2(a1)\n",
    "        a2 = F.leaky_relu(z2, negative_slope= self.n_slope) \n",
    "        a2 = self.dropout(a2)\n",
    "\n",
    "        outputs = self.fc3(a2) #logits - softmax(logit) = p_class\n",
    "        \n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __Data Loading and Preparation__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "#device = xm.xla_device() #Sets up commmunication between CPU and TPU\n",
    "#device = torch.device(\"mps\")\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "#increase learning rate since batch size increase -> more accurate gradient -> can take bigger step\n",
    "#b_size = 128\n",
    "#epochs = 20\n",
    "\n",
    "'''Transformation function object applies transformations in list sequentially (e.g. images into tensors \n",
    "-> normalize). Normalizing/condensing pixels to 0-1 to make model focuses more on relationships instead\n",
    "of brightness levels.'''\n",
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
    "\n",
    "#like list of tuples of (image (tensor), label)...\n",
    "full_train_dataset = datasets.MNIST(root = '../data', train = True, download = False, transform = transform) #true on Colab\n",
    "\n",
    "full_train_loader = DataLoader(full_train_dataset, hypers.b_size, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __Model Evaluation and Selection__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### __K-folds Cross Validation__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#already ensure validation only used once per sample; random_state like seed; generates train/validation indices per fold\n",
    "def k_folds(full_train_dataset, l_rate = hypers.l_rate, b_size = hypers.b_size, epochs = hypers.epochs, drop_rate = hypers.drop_rate, \n",
    "            hidden_one = hypers.hidden_one, hidden_two = hypers.hidden_two, n_slope = hypers.n_slope):\n",
    "    kf = KFold(n_splits=5, shuffle=True, random_state=20)\n",
    "\n",
    "    validation_accuracies = []\n",
    "    #enumerate for fold\n",
    "    for fold, (train_index, val_index) in enumerate(kf.split(full_train_dataset)):\n",
    "        if (fold == 0):\n",
    "            print(f'Fold {fold + 1}')\n",
    "        else:\n",
    "            print(f'\\nFold {fold + 1}')\n",
    "\n",
    "        #train_index: [1,3] -> take images with these indices\n",
    "        train_subset = Subset(full_train_dataset, train_index)\n",
    "        val_subset = Subset(full_train_dataset, val_index)\n",
    "\n",
    "        train_loader = DataLoader(train_subset, batch_size=b_size, shuffle=True)\n",
    "        val_loader = DataLoader(val_subset, batch_size=512, shuffle=False)\n",
    "\n",
    "        model = NumberNN(hidden_one, hidden_two, drop_rate, n_slope).to(device) #ensures parameters reset and each fold is independent\n",
    "        \n",
    "        cross_entropy = nn.CrossEntropyLoss()\n",
    "        adam = optim.Adam(model.parameters(), lr=l_rate, betas=(0.9, 0.999), eps=1e-08) #forget info from EWMA\n",
    "        \n",
    "\n",
    "\n",
    "        last_epoch_val_accuracy = train_model(model, train_loader, optimizer = adam, loss_f = cross_entropy, num_epochs = epochs, val_loader = val_loader)\n",
    "\n",
    "        validation_accuracies.append(last_epoch_val_accuracy)\n",
    "\n",
    "    mean_val_accuracy = np.mean(validation_accuracies)\n",
    "    #Use average validation accuracy across all folds to evaluate model\n",
    "\n",
    "    print(f'\\n\\033[1mMean Validation Accuracy Across All Folds: {mean_val_accuracy:.2f}%\\033[0m\\n')\n",
    "    return mean_val_accuracy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### __Hyperparameter Tuning with Gridsearch__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#All other hyperparameters previously tested\n",
    "hyperdict = {\n",
    "    'drop_rate': [0, 0.2]\n",
    "}\n",
    "\n",
    "best_hypers = None\n",
    "best_mean_val_accuracy = 0\n",
    "\n",
    "#iterable - generates on the fly to save memory; generates all possible dictionary combos\n",
    "for hypercombo in ParameterGrid(hyperdict): \n",
    "    #sometimes you save model also\n",
    "    print(hypercombo)\n",
    "    \n",
    "    mean_val_accuracy = k_folds(full_train_dataset, drop_rate = hypercombo['drop_rate'])\n",
    "\n",
    "    if (mean_val_accuracy > best_mean_val_accuracy):\n",
    "        best_mean_val_accuracy = mean_val_accuracy\n",
    "        best_hypers = hypercombo\n",
    "\n",
    "print(f'Best Hyperparameters: {best_hypers}, Best Mean Validation Accuracy:{best_mean_val_accuracy: .2f}%')\n",
    "#Mean Validation Accuracy Across All Folds: 94.51% zero dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __Training the Model__ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, optimizer, loss_f, num_epochs, val_loader = None):\n",
    "    model.train() #prevents weird stuff (e.g. optimizer being weird)\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "\n",
    "        #each iteration is one minibatch of images/labels\n",
    "        for images, labels in train_loader: \n",
    "            images, labels = images.to(device), labels.to(device) #moves the tensor from virtual machine memory (mapped to RAM of server) to TPU memory\n",
    "\n",
    "            '''Each parameter has its own special tensor (created per layer) \n",
    "            with a number for its partial derivative in .grad attribute of tensor; step() accesses these. Zeroing\n",
    "            out graident ignores .grad and recalculates partials in backprop.'''\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            outputs = model(images) #passes this to forward\n",
    "\n",
    "            #numclasses is components; ex. 4 = [0,0,0,1,0,0,...]\n",
    "            target = F.one_hot(labels, num_classes=10).float()\n",
    "\n",
    "            loss = loss_f(outputs, target) #tensor with scalar and computation graph\n",
    "            loss.backward() #backward propogation to compute gradient\n",
    "\n",
    "            optimizer.step() #updates model parameters (takes \"step\")\n",
    "            #xm.optimizer_step(optimizer) #ensures gradient calculated first - Colab\n",
    "\n",
    "            #xm.mark_step()\n",
    "\n",
    "\n",
    "        #Print out stats for first and last epoch if doing cross validation\n",
    "        if val_loader and (epoch == 0 or epoch == num_epochs - 1):\n",
    "            train_accuracy = evaluate(model, train_loader)\n",
    "            val_accuracy = evaluate(model, val_loader)\n",
    "\n",
    "            print(f'Epoch {epoch + 1} -- Loss:{loss.item(): .4f}, Training Accuracy:{train_accuracy : .2f}%, Validation Accuracy:{val_accuracy : .2f}%')\n",
    "        \n",
    "            #return final validation accuracy\n",
    "            if (epoch == num_epochs - 1):\n",
    "                return val_accuracy\n",
    "        elif not val_loader and (epoch == 0 or epoch == num_epochs - 1):\n",
    "            print(f'Epoch {epoch + 1} -- Loss:{loss.item(): .4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 -- Loss: 0.1457\n",
      "Epoch 20 -- Loss: 0.0613\n"
     ]
    }
   ],
   "source": [
    "model = NumberNN(hypers.hidden_one, hypers.hidden_two, hypers.drop_rate, hypers.n_slope).to(device)\n",
    "cross_entropy = nn.CrossEntropyLoss()\n",
    "adam = optim.Adam(model.parameters(), hypers.l_rate, betas=(0.9, 0.999), eps=1e-08) \n",
    "\n",
    "train_model(model = model, train_loader = full_train_loader, optimizer = adam, loss_f = cross_entropy, num_epochs = hypers.epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __Saving the Model__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save these parameters in Colab or locally\n",
    "'''Model has state dictionary, a Python dictionary that maps to the weights and \n",
    "biases. Extracts this and stores it in a file with .pth convention by convention to store PyTorch parameters \n",
    "or entire models.'''\n",
    "trained_model = f'mlp-lr{hypers.l_rate}-b{hypers.b_size}-ep{hypers.epochs}'\n",
    "\n",
    "torch.save(model.state_dict(), f'../models/{trained_model}.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#If using Colab, download locally\n",
    "files.download('colabmodel.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Then if using Colab, move to correct folder\n",
    "model_downloads_path = os.path.join(os.path.expanduser(\"~\"), \"Downloads\", \"colabmodel.pth\")\n",
    "\n",
    "model_destination_path = \"/Users/darian/Documents/Coding/deep-learning-mnist/models/colabmodel.pth\"\n",
    "\n",
    "shutil.move(model_downloads_path, model_destination_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __Loading the Saved Model__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model():\n",
    "    # Instantiate the model\n",
    "    model = NumberNN(hypers.hidden_one, hypers.hidden_two, hypers.drop_rate, hypers.n_slope).to(device)\n",
    "    #Update model's parameters to what it was after training\n",
    "    model.load_state_dict(torch.load(f'../models/{trained_model}.pth'))\n",
    "    \n",
    "    '''Sometimes in training some neurons turned off (called dropout) to prevent overfitting, but when testing \n",
    "you want all neurons in model to be used, so eval fixes this as well as other settings for testing so model\n",
    "acts \"normally\"'''\n",
    "    model.eval()\n",
    "\n",
    "    return model\n",
    "\n",
    "model = load_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __Evalulating/Testing the Model__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### __Testing Accuracy for all Testing Data__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, loader):\n",
    "    model.eval()\n",
    "\n",
    "    #Counters\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    '''with - gradient descent will be tracked outside of with block. Not tracking ->\n",
    "    doesn't store unnecessary numbers (eg. intermediate activations) for calculating gradient; \n",
    "    Saves memory and its faster\n",
    "    '''\n",
    "    with torch.no_grad():\n",
    "        for images, labels in loader:\n",
    "            images, labels = images.to(device), labels.to(device) #Colab\n",
    "\n",
    "            outputs = model(images) #self.forward(images), uses __call__\n",
    "            '''Return two tensors in tuple for max values (brightest neurons) and their indices per training \n",
    "            example in mini batch. Convention to store unimportant stuff in _. 1 as parameter input means \n",
    "            find max along dimension 1. Outputs = (batch_size, num_classes). Max outputs the max output neuron\n",
    "            and index (number) per batch.\n",
    "            '''\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "\n",
    "            #Update Counters\n",
    "            total += labels.size(0) #size of first dimension (each minibatch)\n",
    "\n",
    "            #Compare each tensor, which returns new tensor with each component being True or False\n",
    "            #Sum counts the amount of True in the tensor\n",
    "            correct += (predicted == labels).sum().item()\n",
    "        return (correct/total) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 96.36%\n"
     ]
    }
   ],
   "source": [
    "test_dataset = datasets.MNIST(root = '../data', train = False, download = False, transform = transform) #True on Colab\n",
    "\n",
    "test_loader = DataLoader(test_dataset, batch_size = 512, shuffle = False)\n",
    "\n",
    "accuracy = evaluate(model, loader = test_loader)\n",
    "print(f'Test Accuracy:{accuracy : .2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### __Testing Accuracy Per Digit__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def digit_indices(dataset, digit):\n",
    "    indices = []\n",
    "    for index, (image, label) in enumerate(dataset):\n",
    "        if label == digit:\n",
    "            indices.append(index)\n",
    "    return indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy for 3: 98.12%\n",
      "Test Accuracy for 1: 97.53%\n",
      "Test Accuracy for 9: 97.52%\n",
      "Test Accuracy for 2: 96.90%\n",
      "Test Accuracy for 5: 96.86%\n",
      "Test Accuracy for 6: 95.93%\n",
      "Test Accuracy for 4: 95.93%\n",
      "Test Accuracy for 7: 95.53%\n",
      "Test Accuracy for 8: 94.76%\n",
      "Test Accuracy for 0: 94.29%\n"
     ]
    }
   ],
   "source": [
    "accuracies = {}\n",
    "\n",
    "for i in range(0,10):\n",
    "    one_digit_indices = digit_indices(dataset = test_dataset, digit = i)\n",
    "    one_digit_test_set = Subset(test_dataset, one_digit_indices)\n",
    "    one_digit_loader = DataLoader(one_digit_test_set, batch_size= 512, shuffle = False)\n",
    "\n",
    "    accuracy = evaluate(model, loader = one_digit_loader)\n",
    "    accuracies[i] = accuracy\n",
    "\n",
    "sorted_accuracies = sorted(accuracies.items(), key = itemgetter(1), reverse = True)\n",
    "\n",
    "for digit, accuracy in sorted_accuracies:\n",
    "    print(f'Test Accuracy for {digit}:{accuracy: .2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __Inputting a New Image__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pixelate_image(image, pixelation_level):\n",
    "    \n",
    "    '''/ is floor division. Tuples have indices. Resize is changing # of pixels after knowing new \n",
    "    size of image, what makes it appear pixelated is then enlarging image so the few pixels are larger.\n",
    "    Bilinear interpoloation is to get value of each pixel, look at weighted (based on distance) average of \n",
    "    4 nearest pixels of where it would go to determine new value. Then split those big pixels up again \n",
    "    so the size is same as original but looks more pixelated.'''\n",
    "\n",
    "    #open and greyscale image (compress to 1 channel)\n",
    "\n",
    "    new_size = (image.size[0] // pixelation_level, image.size[1] // pixelation_level)\n",
    "    pixelated = image.resize(new_size,resample=Image.BILINEAR)\n",
    "\n",
    "    pixelated = pixelated.resize((28, 28))\n",
    "    \n",
    "    return pixelated\n",
    "\n",
    "#Preprocess using functions of the Image class\n",
    "def preprocess_image(image, pixelation_level):\n",
    "    image = pixelate_image(image, pixelation_level)\n",
    "    image = transform(image)\n",
    "    image = image.unsqueeze(0)  #Add another dimension (batch_size) to image; not used, but specific shape necessary\n",
    "    return image\n",
    "\n",
    "def preprocess_no_pixelation(image):\n",
    "    image = image.resize((28, 28))\n",
    "    image = transform(image)  #images -> pixel values-> normalize\n",
    "    image = image.unsqueeze(0)  #Add another dimension (batch_size) to image; not used, but specific shape necessary\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probability of 6: 100.000%\n",
      "Probability of 4: 0.000%\n",
      "Probability of 8: 0.000%\n",
      "Probability of 2: 0.000%\n",
      "Probability of 5: 0.000%\n",
      "Probability of 9: 0.000%\n",
      "Probability of 0: 0.000%\n",
      "Probability of 7: 0.000%\n",
      "Probability of 1: 0.000%\n",
      "Probability of 3: 0.000%\n",
      "\n",
      "Predicted number: 6\n"
     ]
    }
   ],
   "source": [
    "image = Image.open('../data/test-images/6-1.png').convert('L')\n",
    "\n",
    "#pixelation slightly worse outcome actually\n",
    "pixelation_level = 8\n",
    "pixelated_image_visual = pixelate_image(image, pixelation_level)\n",
    "\n",
    "#image_transformed = preprocess_image(image, pixelation_level)\n",
    "image_transformed = preprocess_no_pixelation(image)\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(image_transformed)\n",
    "    _, predicted = torch.max(outputs, dim = 1)\n",
    "\n",
    "    #softmax takes care of negative outputs\n",
    "    probabilities = F.softmax(outputs, dim = 1) #probabilities[i] that contains softmax for outputs of i+1 image \n",
    "\n",
    "#[(number, probability tensor)...]; enumerate packs into tuple with counter\n",
    "prob_tuples = list(enumerate(probabilities[0])) \n",
    "\n",
    "#to sort by max, extract probability with itemgetter\n",
    "prob_tuples.sort(key = itemgetter(1), reverse = True)\n",
    "\n",
    "for number, probability in prob_tuples:\n",
    "    print(f'Probability of {number}:{probability.item() * 100: .3f}%')\n",
    "\n",
    "print(f'\\nPredicted number: {predicted.item()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __Visualizing the Transformed Image__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.subplot(1,2,1) #create subplot first\n",
    "plt.imshow(image, cmap='gray')\n",
    "plt.axis('off')\n",
    "\n",
    "#dimensions need to be correct\n",
    "plt.subplot(1,2,2)\n",
    "plt.imshow(pixelated_image_visual, cmap='gray')\n",
    "plt.axis('off') \n",
    "\n",
    "plt.show() #not needed for Jupyter"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
