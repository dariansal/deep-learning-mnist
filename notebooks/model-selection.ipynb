{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h2><b><u> Multilayer Perceptron with MNIST Dataset</u></b></h2></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __Importing Libraries, Classes, and Functions__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 564,
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "import torch\n",
    "import torch.nn as nn #provides classes/modules for making neural networks\n",
    "import numpy as np\n",
    "from torch.nn import Linear\n",
    "import torch.nn.functional as F #a module with common nn functions (operations on tensors/high dim matrices; activations)\n",
    "import torch.optim as optim #contains optimization algorithms like SGD\n",
    "from torchvision import datasets, transforms #includes MNIST, transform images -> tensors\n",
    "from torch.utils.data import DataLoader, TensorDataset, Subset, ConcatDataset #allows shuffling and minbatches\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import KFold, ParameterGrid\n",
    "import os\n",
    "import shutil\n",
    "import random\n",
    "import json\n",
    "#!pip install torch-xla\n",
    "#import torch_xla\n",
    "#import torch_xla.core.xla_model as xm #tpu\n",
    "#from google.colab import files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __Initializing the Hyperparameters__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 565,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Hypers:\n",
    "    def __init__(self, hyperpath):\n",
    "        with open(hyperpath, 'r') as file:\n",
    "            hyper_dict = json.load(file)\n",
    "        self.l_rate = hyper_dict['learning_rate']\n",
    "        self.b_size = hyper_dict['batch_size']\n",
    "        self.epochs = hyper_dict['epochs']\n",
    "        self.drop_rate = hyper_dict['dropout_rate']\n",
    "        self.hidden_one = hyper_dict['hidden_one']\n",
    "        self.hidden_two = hyper_dict['hidden_two']\n",
    "        self.n_slope = hyper_dict['n_slope']\n",
    "\n",
    "hypers = Hypers('../config/hyperparameters.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __The Model Blueprint__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 566,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with __call__ object name can be treated as function that calls certain function in class (predefine)\n",
    "#inherits from nn.Module class; many classes for different layers\n",
    "class NumberNN(nn.Module): \n",
    "    def __init__(self, hidden_one, hidden_two, drop_prob, n_slope):\n",
    "        super(NumberNN, self).__init__() #explicitly call parent class constructor to initialize stuff\n",
    "                                    #parent (nn.Module) technically initialized too and tied to child but you only access child\n",
    "        self.fc1 = Linear(in_features = 28**2, out_features = hidden_one) #num features = num neurons in input layer (1 neuron per feature/vector component)\n",
    "        self.fc2 = Linear(hidden_one, hidden_two)\n",
    "        self.fc3 = Linear(hidden_two, 10)\n",
    "        self.n_slope = n_slope\n",
    "\n",
    "        self.dropout = nn.Dropout(p=drop_prob) \n",
    "    \n",
    "    def forward(self, x):\n",
    "        '''MNIST is (batch_size = ..., channels = 1, height = 28, width = 28)\n",
    "        Fully connected layers expect input tensors to be (batch_size, num_features); labels already liked this\n",
    "        Size of first dimension of tensor is batch size, -1 infers dim of the vector/features (784)\n",
    "        '''\n",
    "        x = x.view(-1, 28**2)  # Flatten the input tensor (batch_size, 1, 28, 28) to (batch_size, 784)\n",
    "\n",
    "        z1 = self.fc1(x) #Weight matrix + bias vector\n",
    "        a1 = F.leaky_relu(z1, negative_slope= self.n_slope) \n",
    "        a1 = self.dropout(a1) #Starting dropout in layer 2; don't drop inputs\n",
    "\n",
    "        z2 = self.fc2(a1)\n",
    "        a2 = F.leaky_relu(z2, negative_slope= self.n_slope) \n",
    "        a2 = self.dropout(a2)\n",
    "\n",
    "        outputs = self.fc3(a2) #logits - softmax(logit) = p_class\n",
    "        \n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __Data Loading and Preparation__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 567,
   "metadata": {},
   "outputs": [],
   "source": [
    "#device = xm.xla_device() #Sets up commmunication between CPU and TPU\n",
    "device = torch.device(\"mps\")\n",
    "#device = torch.device(\"cpu\")\n",
    "\n",
    "'''Transformation function object applies transformations in list sequentially (e.g. images into tensors \n",
    "-> normalize). Normalizing/condensing pixels to 0-1 to make model focuses more on relationships instead\n",
    "of brightness levels.'''\n",
    "norm = transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))\n",
    "transform_norm = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
    "\n",
    "#more ahyperparameters\n",
    "aug = transforms.RandomAffine(\n",
    "    degrees= 15,                   #max rotation\n",
    "    translate=(0.05, 0.05),#max_horizontal_shift, max vert shift\n",
    "    scale=(0.9, 1.05),            #scaling range\n",
    "    shear=(-10, 10)            #Distorting\n",
    ")\n",
    "\n",
    "\n",
    "#.2 -> ranges from .8 to 1.2 of orig (1)\n",
    "transform_aug = transforms.Compose([aug, transforms.ColorJitter(brightness=.2, contrast=0.2), \n",
    "                                    transforms.ToTensor(),\n",
    "                                    transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "\n",
    "#like list of tuples of (image (tensor), label)...\n",
    "norm_dataset = datasets.MNIST(root = '../data', train = True, download = False, transform = transform_norm) #true on Colab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __Data Augmentation__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### __Viewing the Current Distribution of Classes__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 558,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1: 6742\n",
      "7: 6265\n",
      "3: 6131\n",
      "2: 5958\n",
      "9: 5949\n",
      "0: 5923\n",
      "6: 5918\n",
      "8: 5851\n",
      "4: 5842\n",
      "5: 5421\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nTest Accuracy for 1: 99.82%\\nTest Accuracy for 6: 98.96%\\nTest Accuracy for 0: 98.88%\\nTest Accuracy for 9: 98.02%\\nTest Accuracy for 8: 97.84%\\nTest Accuracy for 3: 97.82%\\nTest Accuracy for 4: 97.76%\\nTest Accuracy for 7: 97.67%\\nTest Accuracy for 2: 97.09%\\nTest Accuracy for 5: 97.09%\\n'"
      ]
     },
     "execution_count": 558,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freq = {}\n",
    "for label in range(0, 10):\n",
    "    freq[label] = 0\n",
    "\n",
    "for image, label in norm_dataset:\n",
    "    freq[label] += 1\n",
    "\n",
    "#list of tuples\n",
    "freq_sorted = sorted(freq.items(), key = itemgetter(1), reverse = True)\n",
    "for number, freq in freq_sorted:\n",
    "    print(f'{number}: {freq}')\n",
    "\n",
    "'''\n",
    "Test Accuracy for 1: 99.82%\n",
    "Test Accuracy for 6: 98.96%\n",
    "Test Accuracy for 0: 98.88%\n",
    "Test Accuracy for 9: 98.02%\n",
    "Test Accuracy for 8: 97.84%\n",
    "Test Accuracy for 3: 97.82%\n",
    "Test Accuracy for 4: 97.76%\n",
    "Test Accuracy for 7: 97.67%\n",
    "Test Accuracy for 2: 97.09%\n",
    "Test Accuracy for 5: 97.09%\n",
    "'''\n",
    "#5 is fewest, can apply tilting/brightness"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### __Applying the Augmentation__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 568,
   "metadata": {},
   "outputs": [],
   "source": [
    "#transformation applied on the fly when image accessed; every epoch sees dif versions of images\n",
    "aug_dataset = datasets.MNIST(root = '../data', train = True, download = False, transform = transform_aug)\n",
    "\n",
    "#Only for training not k_folds\n",
    "full_train_dataset = ConcatDataset([norm_dataset, aug_dataset])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### __Visualizing the Augmentation__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 560,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "index = 4\n",
    "\n",
    "aug_image, label = aug_dataset[index]\n",
    "aug_image = aug_image.squeeze(0) #remove singleton dimension (provides no new info)\n",
    "\n",
    "image, label = norm_dataset[index]\n",
    "image = image.squeeze(0) #remove singleton dimension (provides no new info)\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow(image, cmap=\"grey\")\n",
    "plt.title(\"Original\")\n",
    "plt.axis(\"off\")\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.title(\"Augmented\")\n",
    "plt.imshow(aug_image, cmap = \"grey\")\n",
    "plt.axis(\"off\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __Model Evaluation and Selection__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### __Creating Constants__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 561,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Constants\n",
    "'''For k-folds cross_validation, the validation set should only consist of normal data, but I also want to include augmented \n",
    "data. Therefore, after finding the val indices with KFold from sci-kit learn, I need to decrease the amount of training indices\n",
    "from norm_dataset and replace them with indices from aug_dataset. Specifically for 10-fold cross validation in MNIST, the validation\n",
    "set will be 10% of the data (6,000 examples), so the training should be 54,000. I am deciding to use 70% of the training data from normal\n",
    "data (37,800 examples) and 30% of the training data from augmented data (16,200 examples)\n",
    "'''\n",
    "len_val = 6000\n",
    "cross_training_num = (len(norm_dataset) - len_val)\n",
    "AUG_TRAIN_INDICES = cross_training_num * .7  #37,800\n",
    "NORM_TRAIN_INDICES = cross_training_num * .3 #16,200"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### __K-folds Cross Validation__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 570,
   "metadata": {},
   "outputs": [],
   "source": [
    "#already ensure validation only used once per sample; random_state like seed; generates train/validation indices per fold\n",
    "def k_folds(full_train_dataset, norm_dataset, aug_dataset, l_rate = hypers.l_rate, b_size = hypers.b_size, epochs = hypers.epochs, drop_rate = hypers.drop_rate, \n",
    "            hidden_one = hypers.hidden_one, hidden_two = hypers.hidden_two, n_slope = hypers.n_slope):\n",
    "    kf = KFold(n_splits=10, shuffle=True, random_state=20)\n",
    "\n",
    "    validation_accuracies = []\n",
    "    '''by default split only works with indices arrays unless it can use length attribute of dataset, which Pytorch provides.\n",
    "    This works too: norm_dataset.__len__(), dunder methods simplify syntax'''\n",
    "    norm_indices = np.arange(len(norm_dataset))\n",
    "\n",
    "\n",
    "    for fold, (norm_train_indices, val_indices) in enumerate(kf.split(norm_indices)):\n",
    "        if (fold == 0):\n",
    "            print(f'Fold {fold + 1}')\n",
    "        else:\n",
    "            print(f'\\nFold {fold + 1}')\n",
    "\n",
    "        #My actual validation\n",
    "        val_subset = Subset(norm_dataset, val_indices)\n",
    "        val_loader = DataLoader(val_subset, batch_size=512, shuffle=False)\n",
    "\n",
    "        mapped_indices = norm_train_indices + len(norm_dataset)\n",
    "        aug_training_indices = np.random.choice(mapped_indices, AUG_TRAIN_INDICES, replace = False)  #maps indices to aug\n",
    "        norm_train_indices = np.random.choice(norm_train_indices, NORM_TRAIN_INDICES, replace = False)  #maps indices to aug\n",
    "\n",
    "    \n",
    "        #aug_training_indices = aug_training_indices.reshape(-1)\n",
    "        norm_train_indices = norm_train_indices.reshape(-1)\n",
    "    \n",
    "        full_train_indices = np.concatenate([aug_training_indices, norm_train_indices])\n",
    "        train_subset = Subset(full_train_dataset, full_train_indices)\n",
    "        #Shuffles order of batches and samples within each batch per epoch\n",
    "        train_loader = DataLoader(train_subset, batch_size=b_size, shuffle=True)\n",
    "        \n",
    "\n",
    "        model = NumberNN(hidden_one, hidden_two, drop_rate, n_slope).to(device) #ensures parameters reset and each fold is independent\n",
    "        \n",
    "        cross_entropy = nn.CrossEntropyLoss()\n",
    "        adam = optim.Adam(model.parameters(), lr=l_rate, betas=(0.9, 0.999), eps=1e-08) #forget info from EWMA\n",
    "        \n",
    "\n",
    "\n",
    "        last_epoch_val_accuracy = train_model(model, train_loader, optimizer = adam, loss_f = cross_entropy, num_epochs = epochs, val_loader = val_loader)\n",
    "\n",
    "        validation_accuracies.append(last_epoch_val_accuracy)\n",
    "\n",
    "    mean_val_accuracy = np.mean(validation_accuracies)\n",
    "    #Use average validation accuracy across all folds to evaluate model\n",
    "\n",
    "    print(f'\\n\\033[1mMean Validation Accuracy Across All Folds: {mean_val_accuracy:.2f}%\\033[0m\\n')\n",
    "    return mean_val_accuracy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### __Hyperparameter Tuning with Gridsearch__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 571,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'l_rate': 0.0005}\n",
      "Fold 1\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "only length-1 arrays can be converted to Python scalars",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[571], line 14\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m hypercombo \u001b[38;5;129;01min\u001b[39;00m ParameterGrid(hyperdict): \n\u001b[1;32m     11\u001b[0m     \u001b[38;5;66;03m#sometimes you save model also\u001b[39;00m\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;28mprint\u001b[39m(hypercombo)\n\u001b[0;32m---> 14\u001b[0m     mean_val_accuracy \u001b[38;5;241m=\u001b[39m \u001b[43mk_folds\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfull_train_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnorm_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maug_dataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (mean_val_accuracy \u001b[38;5;241m>\u001b[39m best_mean_val_accuracy):\n\u001b[1;32m     17\u001b[0m         best_mean_val_accuracy \u001b[38;5;241m=\u001b[39m mean_val_accuracy\n",
      "Cell \u001b[0;32mIn[570], line 22\u001b[0m, in \u001b[0;36mk_folds\u001b[0;34m(full_train_dataset, norm_dataset, aug_dataset, l_rate, b_size, epochs, drop_rate, hidden_one, hidden_two, n_slope)\u001b[0m\n\u001b[1;32m     19\u001b[0m val_subset \u001b[38;5;241m=\u001b[39m Subset(norm_dataset, val_indices)\n\u001b[1;32m     20\u001b[0m val_loader \u001b[38;5;241m=\u001b[39m DataLoader(val_subset, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m512\u001b[39m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m---> 22\u001b[0m mapped_indices \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mnorm_train_indices\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mnorm_dataset\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     23\u001b[0m aug_training_indices \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mchoice(mapped_indices, AUG_TRAIN_INDICES, replace \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m)  \u001b[38;5;66;03m#maps indices to aug\u001b[39;00m\n\u001b[1;32m     24\u001b[0m norm_train_indices \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mchoice(\u001b[38;5;28mint\u001b[39m(norm_train_indices), NORM_TRAIN_INDICES, replace \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m)  \u001b[38;5;66;03m#maps indices to aug\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: only length-1 arrays can be converted to Python scalars"
     ]
    }
   ],
   "source": [
    "#All other hyperparameters previously tested\n",
    "hyperdict = {\n",
    "    'l_rate': [0.0005, 0.001, 0.002]\n",
    "}\n",
    "\n",
    "best_hypers = None\n",
    "best_mean_val_accuracy = 0\n",
    "\n",
    "#iterable - generates on the fly to save memory; generates all possible dictionary combos\n",
    "for hypercombo in ParameterGrid(hyperdict): \n",
    "    #sometimes you save model also\n",
    "    print(hypercombo)\n",
    "    \n",
    "    mean_val_accuracy = k_folds(full_train_dataset, norm_dataset, aug_dataset)\n",
    "\n",
    "    if (mean_val_accuracy > best_mean_val_accuracy):\n",
    "        best_mean_val_accuracy = mean_val_accuracy\n",
    "        best_hypers = hypercombo\n",
    "\n",
    "print(f'\\n\\033[1mBest Hyperparameters: {best_hypers}, Best Mean Validation Accuracy:{best_mean_val_accuracy: .2f}%\\033[0m')\n",
    "#Always tune; batch size and learning rate relationship not always linear\n",
    "# Best Hyperparameters: {'l_rate': 0.001}, Mean Validation Accuracy: 97.59% - no augmentation\n",
    "#Same hypers with augmentation - Mean Val: 98%\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __Training the Model__ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, optimizer, loss_f, num_epochs, val_loader = None):\n",
    "    model.train() #prevents weird stuff (e.g. optimizer being weird)\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "\n",
    "        #each iteration is one minibatch of images/labels\n",
    "        for images, labels in train_loader: \n",
    "            images, labels = images.to(device), labels.to(device) #moves the tensor from virtual machine memory (mapped to RAM of server) to TPU memory\n",
    "\n",
    "            '''Each parameter has its own special tensor (created per layer) \n",
    "            with a number for its partial derivative in .grad attribute of tensor; step() accesses these. Zeroing\n",
    "            out graident ignores .grad and recalculates partials in backprop.'''\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            outputs = model(images) #passes this to forward\n",
    "\n",
    "            #numclasses is components; ex. 4 = [0,0,0,1,0,0,...]\n",
    "            target = F.one_hot(labels, num_classes=10).float()\n",
    "\n",
    "            loss = loss_f(outputs, target) #tensor with scalar and computation graph\n",
    "            loss.backward() #backward propogation to compute gradient\n",
    "\n",
    "            optimizer.step() #updates model parameters (takes \"step\")\n",
    "            #xm.optimizer_step(optimizer) #ensures gradient calculated first - Colab\n",
    "\n",
    "            #xm.mark_step()\n",
    "\n",
    "\n",
    "        #Print out stats for first and last epoch if doing cross validation\n",
    "        if val_loader and (epoch == 0 or epoch == num_epochs - 1):\n",
    "            train_accuracy = evaluate(model, train_loader)\n",
    "            val_accuracy = evaluate(model, val_loader)\n",
    "\n",
    "            print(f'Epoch {epoch + 1} -- Loss:{loss.item(): .4f}, Training Accuracy:{train_accuracy : .2f}%, Validation Accuracy:{val_accuracy : .2f}%')\n",
    "        \n",
    "            #return final validation accuracy\n",
    "            if (epoch == num_epochs - 1):\n",
    "                return val_accuracy\n",
    "        elif not val_loader and (epoch == 0 or epoch == num_epochs - 1):\n",
    "            print(f'Epoch {epoch + 1} -- Loss:{loss.item(): .4f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### __Getting Proper Proportions of Augmented and Original Data__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Need custom dataset class with transform attribute\n",
    "class Data:\n",
    "    def __init__(self, images, labels, transform):\n",
    "        self.images = images\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self): #subset has length\n",
    "        return len(self.images)\n",
    "    \n",
    "    def __getitem__(self, index): #works with DataLoader, lets me do dataset[index]; SubSet has this implemented but not with transform\n",
    "        sample = self.images[index]\n",
    "        sample_transformed = self.transform(sample) #calling __call__ method of Compose: for t in self.transforms, x = t(x)\n",
    "        label = self.labels[index] \n",
    "        return sample_transformed, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract(data):\n",
    "    images = []\n",
    "    labels = []\n",
    "    for image, label in data:\n",
    "        images.append(image)\n",
    "        labels.append(label)\n",
    "    return images, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnboundLocalError",
     "evalue": "local variable 'sample' referenced before assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[553], line 17\u001b[0m\n\u001b[1;32m     14\u001b[0m aug_indices \u001b[38;5;241m=\u001b[39m all_indices[num_norm_indices:]\n\u001b[1;32m     16\u001b[0m norm_subset \u001b[38;5;241m=\u001b[39m Subset(norm_dataset, norm_indices)\n\u001b[0;32m---> 17\u001b[0m norm_images, norm_labels \u001b[38;5;241m=\u001b[39m \u001b[43mextract\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnorm_subset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m norm_dataset \u001b[38;5;241m=\u001b[39m Data(norm_images, norm_labels, transform_norm)\n\u001b[1;32m     20\u001b[0m aug_subset \u001b[38;5;241m=\u001b[39m Subset(norm_dataset, aug_indices)\n",
      "Cell \u001b[0;32mIn[550], line 4\u001b[0m, in \u001b[0;36mextract\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m      2\u001b[0m images \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m      3\u001b[0m labels \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m image, label \u001b[38;5;129;01min\u001b[39;00m data:\n\u001b[1;32m      5\u001b[0m     images\u001b[38;5;241m.\u001b[39mappend(image)\n\u001b[1;32m      6\u001b[0m     labels\u001b[38;5;241m.\u001b[39mappend(label)\n",
      "File \u001b[0;32m~/Documents/Coding/deep-learning-mnist/myenv/lib/python3.10/site-packages/torch/utils/data/dataset.py:411\u001b[0m, in \u001b[0;36mSubset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m    409\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(idx, \u001b[38;5;28mlist\u001b[39m):\n\u001b[1;32m    410\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindices[i] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m idx]]\n\u001b[0;32m--> 411\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindices\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\n",
      "Cell \u001b[0;32mIn[542], line 12\u001b[0m, in \u001b[0;36mData.__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, index): \u001b[38;5;66;03m#works with DataLoader, lets me do dataset[index]; SubSet has this implemented but not with transform\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m     sample \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform(\u001b[43msample\u001b[49m[index]) \u001b[38;5;66;03m#calling __call__ method of Compose: for t in self.transforms, x = t(x)\u001b[39;00m\n\u001b[1;32m     13\u001b[0m     label \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlabels[index] \n\u001b[1;32m     14\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m sample, label\n",
      "\u001b[0;31mUnboundLocalError\u001b[0m: local variable 'sample' referenced before assignment"
     ]
    }
   ],
   "source": [
    "#Experimenting differnet, more adjustable way than the k-folds\n",
    "#There's definitely an easier way than this\n",
    "prop_augment = 0.3\n",
    "\n",
    "norm_dataset.transform = None\n",
    "len_norm = len(norm_dataset)\n",
    "num_norm_indices = int(len_norm * (1 - prop_augment))\n",
    "num_aug_indices = int(len_norm * prop_augment)\n",
    "\n",
    "\n",
    "all_indices = np.random.permutation(len_norm) #random array of numbers 0 to n-1\n",
    "\n",
    "norm_indices = all_indices[0:num_norm_indices]\n",
    "aug_indices = all_indices[num_norm_indices:]\n",
    "\n",
    "norm_subset = Subset(norm_dataset, norm_indices)\n",
    "norm_images, norm_labels = extract(norm_subset)\n",
    "norm_dataset = Data(norm_images, norm_labels, transform_norm)\n",
    "\n",
    "aug_subset = Subset(norm_dataset, aug_indices)\n",
    "aug_images, aug_labels = extract(norm_subset)\n",
    "aug_dataset = Data(aug_images, aug_labels, transform_aug)\n",
    "\n",
    "actual_train_data = ConcatDataset([norm_dataset, aug_dataset])\n",
    "actual_train_loader = DataLoader(actual_train_data, batch_size = hypers.b_size, shuffle = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### __Training__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 -- Loss: 0.3582\n",
      "Epoch 20 -- Loss: 0.0101\n"
     ]
    }
   ],
   "source": [
    "model = NumberNN(hypers.hidden_one, hypers.hidden_two, hypers.drop_rate, hypers.n_slope).to(device)\n",
    "cross_entropy = nn.CrossEntropyLoss()\n",
    "adam = optim.Adam(model.parameters(), hypers.l_rate, betas=(0.9, 0.999), eps=1e-08) \n",
    "\n",
    "train_model(model = model, train_loader = actual_train_loader, optimizer = adam, loss_f = cross_entropy, num_epochs = hypers.epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __Saving the Model__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save these parameters in Colab or locally\n",
    "'''Model has state dictionary, a Python dictionary that maps to the weights and \n",
    "biases. Extracts this and stores it in a file with .pth convention by convention to store PyTorch parameters \n",
    "or entire models.'''\n",
    "trained_model = f'mlp-lr{hypers.l_rate}-b{hypers.b_size}-ep{hypers.epochs}'\n",
    "\n",
    "torch.save(model.state_dict(), f'../models/{trained_model}.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#If using Colab, download locally\n",
    "files.download('colabmodel.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Then if using Colab, move to correct folder\n",
    "model_downloads_path = os.path.join(os.path.expanduser(\"~\"), \"Downloads\", \"colabmodel.pth\")\n",
    "\n",
    "model_destination_path = \"/Users/darian/Documents/Coding/deep-learning-mnist/models/colabmodel.pth\"\n",
    "\n",
    "shutil.move(model_downloads_path, model_destination_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __Loading the Saved Model__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model():\n",
    "    # Instantiate the model\n",
    "    model = NumberNN(hypers.hidden_one, hypers.hidden_two, hypers.drop_rate, hypers.n_slope).to(device)\n",
    "    #Update model's parameters to what it was after training\n",
    "    model.load_state_dict(torch.load(f'../models/{trained_model}.pth'))\n",
    "    \n",
    "    '''Sometimes in training some neurons turned off (called dropout) to prevent overfitting, but when testing \n",
    "you want all neurons in model to be used, so eval fixes this as well as other settings for testing so model\n",
    "acts \"normally\"'''\n",
    "    model.eval()\n",
    "\n",
    "    return model\n",
    "\n",
    "model = load_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __Evalulating/Testing the Model__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### __Testing Accuracy for all Testing Data__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, loader):\n",
    "    model.eval()\n",
    "\n",
    "    #Counters\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    '''with - gradient descent will be tracked outside of with block. Not tracking ->\n",
    "    doesn't store unnecessary numbers (eg. intermediate activations) for calculating gradient; \n",
    "    Saves memory and its faster\n",
    "    '''\n",
    "    with torch.no_grad():\n",
    "        for images, labels in loader:\n",
    "            images, labels = images.to(device), labels.to(device) \n",
    "\n",
    "            outputs = model(images) #self.forward(images), uses __call__\n",
    "            '''Return two tensors in tuple for max values (brightest neurons) and their indices per training \n",
    "            example in mini batch. Convention to store unimportant stuff in _. 1 as parameter input means \n",
    "            find max along dimension 1. Outputs = (batch_size, num_classes). Max outputs the max output neuron\n",
    "            and index (number) per batch.\n",
    "            '''\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "\n",
    "            #Update Counters\n",
    "            total += labels.size(0) #size of first dimension (each minibatch)\n",
    "\n",
    "            #Compare each tensor, which returns new tensor with each component being True or False\n",
    "            #Sum counts the amount of True in the tensor\n",
    "            correct += (predicted == labels).sum().item()\n",
    "        return (correct/total) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 98.12%\n"
     ]
    }
   ],
   "source": [
    "test_dataset = datasets.MNIST(root = '../data', train = False, download = False, transform = transform) #True on Colab\n",
    "\n",
    "test_loader = DataLoader(test_dataset, batch_size = 512, shuffle = False)\n",
    "\n",
    "accuracy = evaluate(model, loader = test_loader)\n",
    "print(f'Test Accuracy:{accuracy : .2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### __Testing Accuracy Per Digit__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def digit_indices(dataset, digit):\n",
    "    indices = []\n",
    "    for index, (image, label) in enumerate(dataset):\n",
    "        if label == digit:\n",
    "            indices.append(index)\n",
    "    return indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy for 1: 99.82%\n",
      "Test Accuracy for 6: 98.96%\n",
      "Test Accuracy for 0: 98.88%\n",
      "Test Accuracy for 9: 98.02%\n",
      "Test Accuracy for 8: 97.84%\n",
      "Test Accuracy for 3: 97.82%\n",
      "Test Accuracy for 4: 97.76%\n",
      "Test Accuracy for 7: 97.67%\n",
      "Test Accuracy for 2: 97.09%\n",
      "Test Accuracy for 5: 97.09%\n"
     ]
    }
   ],
   "source": [
    "accuracies = {}\n",
    "\n",
    "for i in range(0,10):\n",
    "    one_digit_indices = digit_indices(dataset = test_dataset, digit = i)\n",
    "    one_digit_test_set = Subset(test_dataset, one_digit_indices)\n",
    "    one_digit_loader = DataLoader(one_digit_test_set, batch_size= 512, shuffle = False)\n",
    "\n",
    "    accuracy = evaluate(model, loader = one_digit_loader)\n",
    "    accuracies[i] = accuracy\n",
    "\n",
    "sorted_accuracies = sorted(accuracies.items(), key = itemgetter(1), reverse = True)\n",
    "\n",
    "for digit, accuracy in sorted_accuracies:\n",
    "    print(f'Test Accuracy for {digit}:{accuracy: .2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __Inputting a New Image__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pixelate_image(image, pixelation_level):\n",
    "    \n",
    "    '''/ is floor division. Tuples have indices. Resize is changing # of pixels after knowing new \n",
    "    size of image, what makes it appear pixelated is then enlarging image so the few pixels are larger.\n",
    "    Bilinear interpoloation is to get value of each pixel, look at weighted (based on distance) average of \n",
    "    4 nearest pixels of where it would go to determine new value. Then split those big pixels up again \n",
    "    so the size is same as original but looks more pixelated.'''\n",
    "\n",
    "    #open and greyscale image (compress to 1 channel)\n",
    "\n",
    "    new_size = (image.size[0] // pixelation_level, image.size[1] // pixelation_level)\n",
    "    pixelated = image.resize(new_size,resample=Image.BILINEAR)\n",
    "\n",
    "    pixelated = pixelated.resize((28, 28))\n",
    "    \n",
    "    return pixelated\n",
    "\n",
    "#Preprocess using functions of the Image class\n",
    "def preprocess_image(image, pixelation_level):\n",
    "    image = pixelate_image(image, pixelation_level)\n",
    "    image = transform(image)  #images -> pixel values-> normalize\n",
    "    image = image.unsqueeze(0)  #Add singleton dimension (batch_size) to image; not used, but specific shape necessary for model\n",
    "    return image\n",
    "\n",
    "def preprocess_no_pixelation(image):\n",
    "    image = image.resize((28, 28))\n",
    "    image = transform(image) \n",
    "    image = image.unsqueeze(0)\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probability of 6: 99.934%\n",
      "Probability of 5: 0.021%\n",
      "Probability of 8: 0.017%\n",
      "Probability of 2: 0.016%\n",
      "Probability of 1: 0.013%\n",
      "Probability of 4: 0.000%\n",
      "Probability of 3: 0.000%\n",
      "Probability of 0: 0.000%\n",
      "Probability of 7: 0.000%\n",
      "Probability of 9: 0.000%\n",
      "\n",
      "Predicted number: 6\n"
     ]
    }
   ],
   "source": [
    "image = Image.open('../data/test-images/6-thin.png').convert('L')\n",
    "\n",
    "#pixelation slightly worse outcome because its different noise than model experienced in training\n",
    "pixelation_level = 1\n",
    "pixelated_image_visual = pixelate_image(image, pixelation_level)\n",
    "\n",
    "image_transformed = preprocess_image(image, pixelation_level).to(device)\n",
    "#image_transformed = preprocess_no_pixelation(image).to(device)\n",
    "with torch.no_grad():\n",
    "    outputs = model(image_transformed)\n",
    "    _, predicted = torch.max(outputs, dim = 1)\n",
    "\n",
    "    #softmax takes care of negative outputs\n",
    "    probabilities = F.softmax(outputs, dim = 1) #probabilities[i] that contains softmax for outputs of i+1 image \n",
    "\n",
    "#[(number, probability tensor)...]; enumerate packs into tuple with counter\n",
    "prob_tuples = list(enumerate(probabilities[0])) \n",
    "\n",
    "#to sort by max, extract probability with itemgetter\n",
    "prob_tuples.sort(key = itemgetter(1), reverse = True)\n",
    "\n",
    "for number, probability in prob_tuples:\n",
    "    print(f'Probability of {number}:{probability.item() * 100: .3f}%')\n",
    "\n",
    "print(f'\\nPredicted number: {predicted.item()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __Visualizing the Transformed Image__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.subplot(1,2,1) #create subplot first\n",
    "plt.imshow(image, cmap='gray')\n",
    "plt.axis('off')\n",
    "\n",
    "#dimensions need to be correct\n",
    "plt.subplot(1,2,2)\n",
    "plt.imshow(pixelated_image_visual, cmap='gray')\n",
    "plt.axis('off') \n",
    "\n",
    "plt.show() #not needed for Jupyter"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
