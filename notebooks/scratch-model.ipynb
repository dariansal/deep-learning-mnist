{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h1><b><u> Multilayer Perceptron for MNIST Dataset From Scratch</u></b></h1></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## __Importing Libraries, Classes, and Functions__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from operator import itemgetter\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## __Loading the Data__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_images(filename):\n",
    "    '''Many file formats use magic numbers so that file readers can easily identify them to verify its reading correct file type. This prevents some errors. Each\n",
    "    pixel is represented from value 0-255 (an unsigned byte/8 bit integer). Metadata is big endian (>, normal) and 4 byte/32 bit integer and stored as 1D array.\n",
    "    file.read(x) takes x bytes from the file and creates an immutable buffer (a bytes object, an array of bytes) and stores it in memory. frombuffer interprets buffer object as array\n",
    "    of bytes of certain data type. It doesn't copy the data just provides a view of the existing data and converts to numpy array.'''\n",
    "    with open(filename, 'rb') as f: #read binary\n",
    "        buffer = f.read(16) #metadata\n",
    "        magic, num_images, rows, cols = np.frombuffer(buffer, dtype='>i4') #rows and columns per image (dimensions)\n",
    "        data = np.frombuffer(f.read(), dtype=np.uint8) #read and store remaining data in 1D numpy array; #numpy spaces out elements instead of commas\n",
    "        data = data.reshape(num_images, rows, cols).astype(np.float32) #reshape flat data into 3D, num_image amt of rows x cols arrays\n",
    "    return data\n",
    "\n",
    "def extract_labels(filename):\n",
    "    with open(filename, 'rb') as f:\n",
    "        magic, num_labels = np.frombuffer(f.read(8), dtype='>i4')\n",
    "        labels = np.frombuffer(f.read(), dtype=np.uint8)\n",
    "    return labels\n",
    "\n",
    "def load_mnist():\n",
    "    data_path = '../data/MNIST/raw'\n",
    "    train_images = extract_images(f'{data_path}/train-images-idx3-ubyte')\n",
    "    train_labels = extract_labels(f'{data_path}/train-labels-idx1-ubyte')\n",
    "    test_images = extract_images(f'{data_path}/t10k-images-idx3-ubyte')\n",
    "    test_labels = extract_labels(f'{data_path}/t10k-labels-idx1-ubyte')\n",
    "    return (train_images, train_labels), (test_images, test_labels) #each dataset is tuple of numpy arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train images shape: (60000, 28, 28)\n",
      "Train labels shape: (60000,)\n",
      "Test images shape: (10000, 28, 28)\n",
      "Test labels shape: (10000,)\n"
     ]
    }
   ],
   "source": [
    "#Comma means tuple containing 1 element otherwise would just be int; numpy arrays come with useful attributes like shape\n",
    "(train_images, train_labels), (test_images, test_labels) = load_mnist()\n",
    "\n",
    "print(f\"Train images shape: {train_images.shape}\")\n",
    "print(f\"Train labels shape: {train_labels.shape}\")\n",
    "print(f\"Test images shape: {test_images.shape}\")\n",
    "print(f\"Test labels shape: {test_labels.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __Creating the Datasets__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(images, labels):\n",
    "    dataset = []\n",
    "    for image, label in zip(images, labels): #zip returns iterator of tuples; iterates through both lists at same time\n",
    "        dataset.append((image, label))\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = create_dataset(train_images,  train_labels)\n",
    "test_dataset = create_dataset(test_images,  test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## __Initializing the Hyperparameters__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Hypers:\n",
    "    def __init__(self, hyperpath):\n",
    "        with open(hyperpath, 'r') as file:\n",
    "            hyper_dict = json.load(file)\n",
    "        self.l_rate = hyper_dict['learning_rate']\n",
    "        self.b_size = hyper_dict['batch_size']\n",
    "        self.epochs = hyper_dict['epochs']\n",
    "        self.drop_rate = hyper_dict['dropout_rate']\n",
    "        self.hidden_one = hyper_dict['hidden_one']\n",
    "        self.hidden_two = hyper_dict['hidden_two']\n",
    "        self.n_slope = hyper_dict['n_slope']\n",
    "        self.augment = hyper_dict['augment']\n",
    "        self.prob_augment = hyper_dict['prob_augment']\n",
    "        self.beta_one = hyper_dict['beta_one']\n",
    "        self.beta_two = hyper_dict['beta_two']\n",
    "        self.degrees = hyper_dict['degrees']\n",
    "        self.trans_horz = hyper_dict['trans_horz']\n",
    "        self.trans_vert = hyper_dict['trans_vert']\n",
    "        self.scale_min = hyper_dict['scale_min']\n",
    "        self.scale_max = hyper_dict['scale_max']\n",
    "        self.shear = hyper_dict['shear']\n",
    "        self.brightness = hyper_dict['brightness']\n",
    "        self.contrast = hyper_dict['contrast']\n",
    "\n",
    "\n",
    "#can feed this into functions that need hypers\n",
    "hypers = Hypers('../config/mlp-scratch-hyperparameters.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __Setting the Seed__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 20\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __Transforming the Data__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def norm_transform(dataset):\n",
    "    for idx, (image, label) in enumerate(dataset):\n",
    "        dataset[idx] = (image/255.0, label)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = norm_transform(train_dataset)\n",
    "test_dataset = norm_transform(test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## __Analyzing the Data__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def distribution(dataset):\n",
    "    freq = {}\n",
    "\n",
    "    for image, label in dataset:\n",
    "        freq[label] = freq.get(label, 0) + 1\n",
    "\n",
    "    #list of tuples\n",
    "    freq_sorted = sorted(freq.items(), key = itemgetter(1), reverse = True)\n",
    "    for number, freq in freq_sorted:\n",
    "        print(f'{number}: {freq}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1: 6742\n",
      "7: 6265\n",
      "3: 6131\n",
      "2: 5958\n",
      "9: 5949\n",
      "0: 5923\n",
      "6: 5918\n",
      "8: 5851\n",
      "4: 5842\n",
      "5: 5421\n"
     ]
    }
   ],
   "source": [
    "distribution(train_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __Defining Functions__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x): #x is vector\n",
    "    x_shifted = x - np.max(x) #ensures exp(x) is in range [0,1]\n",
    "    \n",
    "    sum_exp = np.sum(np.exp(x_shifted))\n",
    "\n",
    "    #np operations create new objects\n",
    "    return np.exp(x_shifted) / sum_exp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy(y_truth, y_pred): \n",
    "\n",
    "    #Replace 0 predicted prob with small value to avoid log(0)\n",
    "    epsilon = 1e-15\n",
    "    y_pred = np.maximum(y_pred, epsilon)\n",
    "    \n",
    "    return -np.sum(y_truth * np.log(y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.73105858, 0.26894142])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ar2 = np.array([0.01,-0.99])\n",
    "softmax(ar2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
