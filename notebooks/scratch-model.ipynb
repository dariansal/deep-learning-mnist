{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h1><b><u> Multilayer Perceptron for MNIST Dataset From Scratch</u></b></h1></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## __Importing Libraries, Classes, and Functions__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from operator import itemgetter\n",
    "import random\n",
    "import json\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## __Loading the Data__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_images(filename):\n",
    "    '''Many file formats use magic numbers so that file readers can easily identify them to verify its reading correct file type. This prevents some errors. Each\n",
    "    pixel is represented from value 0-255 (an unsigned byte/8 bit integer). Metadata is big endian (>, normal) and 4 byte/32 bit integer and stored as 1D array.\n",
    "    file.read(x) takes x bytes from the file and creates an immutable buffer (a bytes object, an array of bytes) and stores it in memory. frombuffer interprets buffer object as array\n",
    "    of bytes of certain data type. It doesn't copy the data just provides a view of the existing data and converts to numpy array.'''\n",
    "    with open(filename, 'rb') as f: #read binary\n",
    "        buffer = f.read(16) #metadata\n",
    "        magic, num_images, rows, cols = np.frombuffer(buffer, dtype='>i4') #rows and columns per image (dimensions)\n",
    "        data = np.frombuffer(f.read(), dtype=np.uint8) #read and store remaining data in 1D numpy array; #numpy spaces out elements instead of commas\n",
    "        data = data.reshape(num_images, rows, cols).astype(np.float32) #reshape flat data into 3D, num_image amt of rows x cols arrays\n",
    "    return data\n",
    "\n",
    "def extract_labels(filename):\n",
    "    with open(filename, 'rb') as f:\n",
    "        magic, num_labels = np.frombuffer(f.read(8), dtype='>i4')\n",
    "        labels = np.frombuffer(f.read(), dtype=np.uint8)\n",
    "    return labels\n",
    "\n",
    "def one_hot_encode(labels, num_classes=10):\n",
    "    return np.eye(num_classes)[labels] #certain row of the identity matrix\n",
    "\n",
    "def load_mnist():\n",
    "    data_path = '../data/MNIST/raw'\n",
    "    train_images = extract_images(f'{data_path}/train-images-idx3-ubyte')\n",
    "    train_labels = extract_labels(f'{data_path}/train-labels-idx1-ubyte')\n",
    "    test_images = extract_images(f'{data_path}/t10k-images-idx3-ubyte')\n",
    "    test_labels = extract_labels(f'{data_path}/t10k-labels-idx1-ubyte')\n",
    "\n",
    "    #Make sure data is right shape for forward pass\n",
    "    train_images_flat = train_images.reshape(60000, 784).T\n",
    "    test_images_flat = test_images.reshape(10000, 784).T\n",
    "\n",
    "    train_labels_one_hot = one_hot_encode(train_labels).T\n",
    "    test_labels_one_hot = one_hot_encode(test_labels).T\n",
    "    \n",
    "    return (train_images_flat, train_labels_one_hot), (test_images_flat, test_labels_one_hot) #each dataset is tuple of numpy arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train images shape: (784, 60000)\n",
      "Train labels shape: (10, 60000)\n",
      "Test images shape: (784, 10000)\n",
      "Test labels shape: (10, 10000)\n"
     ]
    }
   ],
   "source": [
    "#Comma means tuple containing 1 element otherwise would just be int; numpy arrays come with useful attributes like shape\n",
    "(train_images, train_labels), (test_images, test_labels) = load_mnist()\n",
    "\n",
    "print(f\"Train images shape: {train_images.shape}\")\n",
    "print(f\"Train labels shape: {train_labels.shape}\")\n",
    "print(f\"Test images shape: {test_images.shape}\")\n",
    "print(f\"Test labels shape: {test_labels.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __Viewing a Sample__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAD7CAYAAACscuKmAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjkuMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy80BEi2AAAACXBIWXMAAA9hAAAPYQGoP6dpAAAFRElEQVR4nO3dL4vUWxzA4TvXLcJqX9BgVBG2iMWkFjGK0XdgEl+AL8AkKAhisBoXMYlBMJhMohhEMQmWBbXozm2GG87MvbN/ZvfzPPX783cOLB9OOOPMZDqdTv8CDry/93oDwO4QO0SIHSLEDhFihwixQ4TYIULsECF2iFiZ98HJZLKT+wAWMM8HYZ3sECF2iBA7RIgdIsQOEWKHCLFDhNghQuwQIXaIEDtEiB0ixA4RYocIsUOE2CFC7BAhdogQO0SIHSLEDhFihwixQ4TYIULsECF2iBA7RIgdIsQOEWKHCLFDhNghQuwQIXaIEDtEiB0ixA4RYocIsUOE2CFC7BAhdogQO0SIHSLEDhFihwixQ4TYIWJlrzfAf7eyMvvPtrq6Opxfvnx5OD937tzMNU6dOjWcb25uznzHyNOnT2c+8/nz5+H8+fPnC+3hIHGyQ4TYIULsECF2iBA7RIgdIsQOEWKHiMl0Op3O9eBkstN7YU4vX76c+cz58+d3YSd778WLF8P5hQsXdmkne2uejJ3sECF2iBA7RIgdIsQOEWKHCLFDhHv2fej06dMzn7l169ZCa6yvr2/LMyNbW1vD+ZMnT2a+4/bt28P5u3fv/suW9i337MAfYocIsUOE2CFC7BAhdogQO0S4Z486fPjwcP7gwYOZ77h+/fpCe7hx48Zwfu/evYXeX+KeHfhD7BAhdogQO0SIHSLEDhFih4iVvd4AO+PQoUPD+cbGxnB+8eLFmWv8/v17OL9///5w/ujRo5lrsH2c7BAhdogQO0SIHSLEDhFihwixQ4TYIcKHag6oK1euDOfzfGhmlps3bw7nd+/eXXgNto+THSLEDhFihwixQ4TYIULsECF2iPAjEQfU69evh/OzZ88O52/fvp25xqx3/PjxY+Y72B5+JAL4Q+wQIXaIEDtEiB0ixA4RYocI/599H1pbW9uWZ0YeP3488xn36PuLkx0ixA4RYocIsUOE2CFC7BAhdohwz74PPXz4cOYzx44dW2iNZ8+eLfTvWT5OdogQO0SIHSLEDhFihwixQ4TYIULsEOFDNUvoyJEjw/mZM2cWXuPTp0/D+YcPHxZeg+XiZIcIsUOE2CFC7BAhdogQO0SIHSLcsy+hq1evDufHjx9feI2NjY3h/OfPnwuvwXJxskOE2CFC7BAhdogQO0SIHSLEDhHu2ZfQtWvXdnyNV69e7fgaLBcnO0SIHSLEDhFihwixQ4TYIULsEOGefQmtrq7u+Brv37/f8TVYLk52iBA7RIgdIsQOEWKHCLFDhNghQuwQIXaIEDtEiB0ixA4RYocIsUOE2CFC7BDhyyv2wNGjR4fzkydPLrzGt2/fhvMvX74svAb7i5MdIsQOEWKHCLFDhNghQuwQIXaIcM++B6bT6XD+69evhdfY3Nwczr9+/brwGuwvTnaIEDtEiB0ixA4RYocIsUOE2CHCPfse+P79+3D+8ePH4XxtbW07t0OEkx0ixA4RYocIsUOE2CFC7BAhdogQO0T4UM0e2NraGs6348sr4N+c7BAhdogQO0SIHSLEDhFihwixQ4R79gPqxIkTw/n6+vpw/ubNm+3bDEvByQ4RYocIsUOE2CFC7BAhdogQO0S4Z4+6c+fOcH7p0qVd2gm7xckOEWKHCLFDhNghQuwQIXaIEDtETKbT6XSuByeTnd4L8D/Nk7GTHSLEDhFihwixQ4TYIULsECF2iBA7RIgdIsQOEWKHCLFDhNghQuwQIXaIEDtEzP0jEXN+xwWwpJzsECF2iBA7RIgdIsQOEWKHCLFDhNghQuwQ8Q9rnKksO6CQyQAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 600x300 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "index = random.randrange(0, train_images.shape[1])\n",
    "\n",
    "plt.figure(figsize=(6, 3))\n",
    "plt.imshow(train_images[:, index].reshape(28,28), cmap = 'gray')\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## __Initializing the Hyperparameters__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Hypers:\n",
    "    def __init__(self, hyperpath):\n",
    "        with open(hyperpath, 'r') as file:\n",
    "            hyper_dict = json.load(file)\n",
    "        self.lr = hyper_dict['learning_rate']\n",
    "        self.b_size = hyper_dict['batch_size']\n",
    "        self.n_slope = hyper_dict['n_slope']\n",
    "        self.epochs = hyper_dict['epochs']\n",
    "        self.hidden = hyper_dict['hidden']\n",
    "        self.augment = hyper_dict['augment']\n",
    "        self.prob_augment = hyper_dict['prob_augment']\n",
    "        self.degrees = hyper_dict['degrees']\n",
    "        self.trans_horz = hyper_dict['trans_horz']\n",
    "        self.trans_vert = hyper_dict['trans_vert']\n",
    "        self.scale_min = hyper_dict['scale_min']\n",
    "        self.scale_max = hyper_dict['scale_max']\n",
    "        self.shear = hyper_dict['shear']\n",
    "        self.brightness = hyper_dict['brightness']\n",
    "        self.contrast = hyper_dict['contrast']\n",
    "\n",
    "\n",
    "#can feed this into functions that need hypers\n",
    "hypers = Hypers('../config/mlp-scratch-hyperparameters.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __Defining Functions for Scalars/Vectors/Matrices__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x): #x is vector\n",
    "    # Subtract max for numerical stability, keeping dims for proper broadcasting\n",
    "    x_shifted = x - np.max(x, axis=0, keepdims=True)\n",
    "    \n",
    "    # Compute exponentials\n",
    "    exp_x = np.exp(x_shifted)\n",
    "    \n",
    "    # Compute softmax\n",
    "    return exp_x / np.sum(exp_x, axis=0, keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(z):\n",
    "    return np.maximum(z, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drelu_dz(z):\n",
    "    return np.where(z <= 0, 0.0, 1.0).astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = norm_transform(train_dataset)\n",
    "test_dataset = norm_transform(test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## __Analyzing the Data__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def distribution(dataset):\n",
    "    freq = {}\n",
    "\n",
    "    for image, label in dataset:\n",
    "        freq[label] = freq.get(label, 0) + 1\n",
    "\n",
    "    #list of tuples\n",
    "    freq_sorted = sorted(freq.items(), key = itemgetter(1), reverse = True)\n",
    "    for number, freq in freq_sorted:\n",
    "        print(f'{number}: {freq}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1: 6742\n",
      "7: 6265\n",
      "3: 6131\n",
      "2: 5958\n",
      "9: 5949\n",
      "0: 5923\n",
      "6: 5918\n",
      "8: 5851\n",
      "4: 5842\n",
      "5: 5421\n"
     ]
    }
   ],
   "source": [
    "distribution(train_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __Defining Functions__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x): #x is vector\n",
    "    x_shifted = x - np.max(x) #ensures exp(x) is in range [0,1]\n",
    "    \n",
    "    sum_exp = np.sum(np.exp(x_shifted))\n",
    "\n",
    "    #np operations create new objects\n",
    "    return np.exp(x_shifted) / sum_exp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy(y_truth, y_pred): \n",
    "\n",
    "    #Replace 0 predicted prob with small value to avoid log(0)\n",
    "    epsilon = 1e-15\n",
    "    y_pred = np.maximum(y_pred, epsilon)\n",
    "    \n",
    "    return -np.sum(y_truth * np.log(y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(z):\n",
    "    return 0 if z <=0 else z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def he_init(shape):\n",
    "    #input, hidden\n",
    "    fan_in, fan_out = shape\n",
    "    \n",
    "    #scaling factor; 2 for ReLU\n",
    "    scaling = np.sqrt(2/fan_in)\n",
    "\n",
    "    #create matrix sampled from Gaussian and multiply by scaling factor\n",
    "    return np.random.randn(fan_in, fan_out) * scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "incomplete input (1735320313.py, line 9)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[69], line 9\u001b[0;36m\u001b[0m\n\u001b[0;31m    \u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m incomplete input\n"
     ]
    }
   ],
   "source": [
    "class MultiLayerPerceptron:\n",
    "    def __init__(self, batch_size = 64, input_dim = 784, hidden_dim = 1024, output = 10):\n",
    "        W = he_init((input_dim, hidden_dim)) \n",
    "        X = np.zeros(input_dim, batch_size)\n",
    "\n",
    "\n",
    "    #forward\n",
    "    def __call__(self, batch):\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
