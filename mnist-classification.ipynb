{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h2><b><u> Multilayer Perceptron with MNIST Dataset</u></b></h2></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __Importing Libraries, Classes, and Functions__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "import torch\n",
    "import torch.nn as nn #provides classes/modules for making neural networks\n",
    "import numpy as np\n",
    "from torch.nn import Linear\n",
    "import torch.nn.functional as F #a module with common nn functions (operations on tensors/high dim matrices; activations)\n",
    "import torch.optim as optim #contains optimization algorithms like SGD\n",
    "from torchvision import datasets, transforms #includes MNIST, transform images -> tensors\n",
    "from torch.utils.data import DataLoader, TensorDataset, Subset, random_split #allows shuffling and minbatches\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import KFold\n",
    "import os\n",
    "import shutil\n",
    "#import torch_xla\n",
    "#TPU device\n",
    "#import torch_xla.core.xla_model as xm\n",
    "#from google.colab import files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __The Model Blueprint__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Can use GPU, which has lots of cores, instead if available. It is good for parallel processing for tasks\n",
    "like matrix multiplication where job can be split up to calculate each element of new matrix. GPU's also have\n",
    "high bandwidth so they can indirectly have faster time talking to memory. This computer doesn't have cuda so\n",
    "run on Colab.\n",
    "'''\n",
    "\n",
    "#Create a NN class that encapsulates all components of NN, instantiate later\n",
    "# with __call__ object name can be treated as function that calls certain function in class (predefine)\n",
    "#inherits from nn.Module class; many classes for different layers\n",
    "class NumberNN(nn.Module): \n",
    "    def __init__(self):\n",
    "        super(NumberNN, self).__init__() #explicitly call parent class constructor to initialize stuff\n",
    "                                    #parent technically initialized too and tied to child but you only access child\n",
    "        self.fc1 = Linear(in_features = 28**2, out_features = 256) #num features = num neurons in input layer (1 neuron per feature/vector component)\n",
    "        self.fc2 = Linear(256, 128)\n",
    "        self.fc3 = Linear(128, 10)\n",
    "    \n",
    "    def forward(self, pixels):\n",
    "        '''MNIST is (batch_size = ..., channels = 1, height = 28, width = 28)\n",
    "        Fully connected layers expect input tensors to be (batch_size, num_features); labels already liked this\n",
    "        Size of first dimension of tensor is batch size, -1 infers dim of the vector/features (784)\n",
    "        '''\n",
    "        pixels = pixels.view(-1, 28**2)  # Flatten the input tensor (batch_size, 1, 28, 28) to (batch_size, 784)\n",
    "        \n",
    "        pixels = F.leaky_relu(self.fc1(pixels), negative_slope= 0.02)\n",
    "        pixels = F.leaky_relu(self.fc2(pixels), negative_slope= 0.02) \n",
    "        pixels = self.fc3(pixels) # Output layer (logits)\n",
    "        return pixels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __Data Loading and Preparation__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "#device = xm.xla_device() Colab\n",
    "\n",
    "#Loss function - making instance of this class to use the functions in it\n",
    "cross_entropy = nn.CrossEntropyLoss()\n",
    "\n",
    "'''Transformation function object applies transformations in list sequentially (e.g. images into tensors \n",
    "-> normalize). Normalizing/condensing pixels to 0-1 to make model focuses more on relationships instead\n",
    "of brightness levels.'''\n",
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
    "\n",
    "#like list of tuples of (image (tensor), label)...\n",
    "full_train_dataset = datasets.MNIST(root = './data', train = True, download = False, transform = transform)\n",
    "\n",
    "full_train_loader = DataLoader(full_train_dataset, batch_size=64, shuffle=True) #numworkers>0 for Colab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __Evaluating the Model Using K-Folds Cross Validation__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "#val_loader is optional\n",
    "def train_model(model, train_loader, optimizer, loss_f, num_epochs, val_loader = None):\n",
    "    for epoch in range(num_epochs):\n",
    "\n",
    "        #each iteration is one minibatch of images/labels\n",
    "        for images, labels in train_loader: \n",
    "            #images, labels = images.to(device), labels.to(device) Colab\n",
    "\n",
    "            '''Each parameter has its own special tensor (created per layer) \n",
    "            with a number for its partial derivative in .grad attribute of tensor; step() accesses these. Zeroing\n",
    "            out graident ignores .grad and recalculates partials in backprop.'''\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            outputs = model(images) #passes this to forward\n",
    "\n",
    "            #numclasses is components; ex. 4 = [0,0,0,1,0,0,...]\n",
    "            target = F.one_hot(labels, num_classes=10).float()\n",
    "\n",
    "            loss = loss_f(outputs, target) #tensor with scalar and computation graph\n",
    "            loss.backward() #backward propogation to compute gradiaent\n",
    "            optimizer.step() #updates model parameters (takes \"step\")\n",
    "            #xm.optimizer_step(optimizer) Colab\n",
    "\n",
    "\n",
    "        #Print out stats for first and last epoch if doing cross validation\n",
    "        if val_loader and (epoch == 0 or epoch == num_epochs - 1):\n",
    "            train_accuracy = evaluate(model, train_loader)\n",
    "            val_accuracy = evaluate(model, val_loader)\n",
    "\n",
    "            print(f'Epoch {epoch + 1} -- Loss:{loss.item(): .4f}, Training Accuracy:{train_accuracy : .2f}%, Validation Accuracy:{val_accuracy : .2f}%')\n",
    "        \n",
    "            #return final validation accuracy\n",
    "            if (epoch == num_epochs - 1):\n",
    "                return val_accuracy\n",
    "        if not val_loader and (epoch == 0 or epoch == num_epochs - 1):\n",
    "            print(f'Epoch {epoch + 1} -- Loss:{loss.item(): .4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1\n",
      "Epoch 1 -- Loss: 0.2449, Training Accuracy: 92.93%, Validation Accuracy: 92.83%\n",
      "Epoch 5 -- Loss: 0.0513, Training Accuracy: 96.51%, Validation Accuracy: 95.72%\n",
      "\n",
      "Fold 2\n",
      "Epoch 1 -- Loss: 0.2785, Training Accuracy: 93.22%, Validation Accuracy: 92.77%\n",
      "Epoch 5 -- Loss: 0.0753, Training Accuracy: 97.87%, Validation Accuracy: 96.97%\n",
      "\n",
      "Fold 3\n",
      "Epoch 1 -- Loss: 0.2107, Training Accuracy: 91.95%, Validation Accuracy: 91.98%\n",
      "Epoch 5 -- Loss: 0.0863, Training Accuracy: 97.26%, Validation Accuracy: 96.46%\n",
      "\n",
      "Fold 4\n",
      "Epoch 1 -- Loss: 0.2207, Training Accuracy: 93.52%, Validation Accuracy: 93.09%\n",
      "Epoch 5 -- Loss: 0.0931, Training Accuracy: 97.42%, Validation Accuracy: 96.38%\n",
      "\n",
      "Fold 5\n",
      "Epoch 1 -- Loss: 0.3430, Training Accuracy: 93.51%, Validation Accuracy: 92.92%\n",
      "Epoch 5 -- Loss: 0.0400, Training Accuracy: 97.35%, Validation Accuracy: 96.23%\n",
      "\u001b[1mMean Validation Accuracy Across All Folds: 96.35%\n"
     ]
    }
   ],
   "source": [
    "#already ensure validation only used once per sample; random_state like seed; generates train/validation indices per fold\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=20)\n",
    "\n",
    "validation_accuracies = []\n",
    "\n",
    "#enumerate for fold\n",
    "for fold, (train_index, val_index) in enumerate(kf.split(full_train_dataset)):\n",
    "    if (fold == 0):\n",
    "        print(f'Fold {fold + 1}')\n",
    "    else:\n",
    "        print(f'\\nFold {fold + 1}')\n",
    "\n",
    "    #train_index: [1,3] -> take images with these indices\n",
    "    train_subset = Subset(full_train_dataset, train_index)\n",
    "    val_subset = Subset(full_train_dataset, val_index)\n",
    "\n",
    "    train_loader = DataLoader(train_subset, batch_size=64, shuffle=True)\n",
    "    val_loader = DataLoader(val_subset, batch_size=64, shuffle=False)\n",
    "\n",
    "    model = NumberNN() #ensures parameters reset and each fold is independent\n",
    "    #device = xm.xla_device() Colab\n",
    "    #model.to(device)\n",
    "\n",
    "    #forget info from EWMA\n",
    "    adam = optim.Adam(model.parameters(), lr=0.001, betas=(0.9, 0.999), eps=1e-08) \n",
    "\n",
    "    last_epoch_val_accuracy = train_model(model = model, train_loader = train_loader, val_loader = val_loader, optimizer = adam, loss_f = cross_entropy, num_epochs = 5)\n",
    "\n",
    "    validation_accuracies.append(last_epoch_val_accuracy)\n",
    "\n",
    "#Use average validation accuracy across all folds to evaluate model\n",
    "mean_val_accuracy = np.mean(validation_accuracies)\n",
    "print(f'\\n\\033[1mMean Validation Accuracy Across All Folds: {mean_val_accuracy:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __Training the Model__ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 -- Loss: 0.1721\n",
      "Epoch 5 -- Loss: 0.1411\n"
     ]
    }
   ],
   "source": [
    "model = NumberNN()\n",
    "adam = optim.Adam(model.parameters(), lr=0.001, betas=(0.9, 0.999), eps=1e-08) \n",
    "\n",
    "train_model(model = model, train_loader = full_train_loader, optimizer = adam, loss_f = cross_entropy, num_epochs = 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __Saving the Model__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save these parameters in Colab or locally\n",
    "'''Model has state dictionary, a Python dictionary that maps to the weights and \n",
    "biases. Extracts this and stores it in a file with .pth convention by convention to store PyTorch parameters \n",
    "or entire models.'''\n",
    "torch.save(model.state_dict(), 'models/complex.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#If using Colab, download locally\n",
    "files.download('models/complex.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Then if using Colab, move to correct folder\n",
    "model_downloads_path = os.path.join(os.path.expanduser(\"~\"), \"Downloads\", \"bestmodel.pth\")\n",
    "\n",
    "model_destination_path = \"/mnt/c/Users/daria/OneDrive/Practice/deep-learning-mnist/models/bestmodel.pth\"\n",
    "\n",
    "shutil.move(model_downloads_path, model_destination_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __Loading the Saved Model__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model():\n",
    "    # Instantiate the model\n",
    "    model = NumberNN()\n",
    "    #Update model's parameters to what it was after training\n",
    "    model.load_state_dict(torch.load('models/complex.pth'))\n",
    "    \n",
    "    '''Sometimes in training some neurons turned off (called dropout) to prevent overfitting, but when testing \n",
    "you want all neurons in model to be used, so eval fixes this as well as other settings for testing so model\n",
    "acts \"normally\"'''\n",
    "    model.eval()\n",
    "\n",
    "    return model\n",
    "\n",
    "model = load_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __Evalulating/Testing the Model__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### __Testing Accuracy for all Testing Data__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, loader):\n",
    "    model.eval()\n",
    "\n",
    "    #Counters\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    '''with - gradient descent will be tracked outside of with block. Not tracking ->\n",
    "    doesn't store unnecessary numbers (eg. intermediate activations) for calculating gradient; \n",
    "    Saves memory and its faster\n",
    "    '''\n",
    "    with torch.no_grad():\n",
    "        for images, labels in loader:\n",
    "            #images, labels = images.to(device), labels.to(device) Colab\n",
    "\n",
    "            outputs = model(images) #self.forward(images), uses __call__\n",
    "            '''Return two tensors in tuple for max values (brightest neurons) and their indices per training \n",
    "            example in mini batch. Convention to store unimportant stuff in _. 1 as parameter input means \n",
    "            find max along dimension 1. Outputs = (batch_size, num_classes). Max outputs the max output neuron\n",
    "            and index (number) per batch.\n",
    "            '''\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "\n",
    "            #Update Counters\n",
    "            total += labels.size(0) #size of first dimension (each minibatch)\n",
    "\n",
    "            #Compare each tensor, which returns new tensor with each component being True or False\n",
    "            #Sum counts the amount of True in the tensor\n",
    "            correct += (predicted == labels).sum().item()\n",
    "        return (correct/total) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 97.13%\n"
     ]
    }
   ],
   "source": [
    "test_dataset = datasets.MNIST(root = './data', train = False, download = False, transform = transform)\n",
    "test_loader = DataLoader(test_dataset, batch_size = 64, shuffle = False)\n",
    "\n",
    "accuracy = evaluate(model, loader = test_loader)\n",
    "print(f'Test Accuracy:{accuracy : .2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### __Testing Accuracy Per Digit__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def digit_indices(dataset, digit):\n",
    "    indices = []\n",
    "    for index, (image, label) in enumerate(dataset):\n",
    "        if label == digit:\n",
    "            indices.append(index)\n",
    "    return indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy for 1: 99.38%\n",
      "Test Accuracy for 0: 98.67%\n",
      "Test Accuracy for 2: 98.35%\n",
      "Test Accuracy for 6: 98.23%\n",
      "Test Accuracy for 8: 97.54%\n",
      "Test Accuracy for 3: 97.52%\n",
      "Test Accuracy for 9: 97.13%\n",
      "Test Accuracy for 5: 96.97%\n",
      "Test Accuracy for 7: 96.69%\n",
      "Test Accuracy for 4: 92.77%\n"
     ]
    }
   ],
   "source": [
    "accuracies = {}\n",
    "\n",
    "for i in range(0,10):\n",
    "    one_digit_indices = digit_indices(dataset = test_dataset, digit = i)\n",
    "    one_digit_test_set = Subset(test_dataset, one_digit_indices)\n",
    "    one_digit_loader = DataLoader(one_digit_test_set, batch_size= 64, shuffle = False)\n",
    "\n",
    "    accuracy = evaluate(model, loader = one_digit_loader)\n",
    "    accuracies[i] = accuracy\n",
    "\n",
    "sorted_accuracies = sorted(accuracies.items(), key = itemgetter(1), reverse = True)\n",
    "\n",
    "for digit, accuracy in sorted_accuracies:\n",
    "    print(f'Test Accuracy for {digit}:{accuracy: .2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __Inputting a New Image__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pixelate_image(image, pixelation_level):\n",
    "    \n",
    "    '''/ is floor division. Tuples have indices. Resize is changing # of pixels after knowing new \n",
    "    size of image, what makes it appear pixelated is then enlarging image so the few pixels are larger.\n",
    "    Bilinear interpoloation is to get value of each pixel, look at weighted (based on distance) average of \n",
    "    4 nearest pixels of where it would go to determine new value. Then split those big pixels up again \n",
    "    so the size is same as original but looks more pixelated.'''\n",
    "\n",
    "    #open and greyscale image (compress to 1 channel)\n",
    "\n",
    "    new_size = (image.size[0] // pixelation_level, image.size[1] // pixelation_level)\n",
    "    pixelated = image.resize(new_size,resample=Image.BILINEAR)\n",
    "\n",
    "    pixelated = pixelated.resize((28, 28))\n",
    "    \n",
    "    return pixelated\n",
    "\n",
    "#Preprocess using functions of the Image class\n",
    "def preprocess_image(image, pixelation_level):\n",
    "    image = pixelate_image(image, pixelation_level)\n",
    "    image = transform(image)\n",
    "    image = image.unsqueeze(0)  #Add another dimension (batch_size) to image; not used, but specific shape necessary\n",
    "    return image\n",
    "\n",
    "def preprocess_no_pixelation(image):\n",
    "    image = image.resize((28, 28))\n",
    "    image = transform(image)  #images -> pixel values-> normalize\n",
    "    image = image.unsqueeze(0)  #Add another dimension (batch_size) to image; not used, but specific shape necessary\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probability of 6: 100.000%\n",
      "Probability of 5: 0.000%\n",
      "Probability of 8: 0.000%\n",
      "Probability of 4: 0.000%\n",
      "Probability of 0: 0.000%\n",
      "Probability of 9: 0.000%\n",
      "Probability of 1: 0.000%\n",
      "Probability of 2: 0.000%\n",
      "Probability of 3: 0.000%\n",
      "Probability of 7: 0.000%\n",
      "\n",
      "Predicted number: 6\n"
     ]
    }
   ],
   "source": [
    "image = Image.open('6.2.png').convert('L')\n",
    "\n",
    "#pixelation slightly worse outcome actually\n",
    "pixelation_level = 1\n",
    "\n",
    "pixelated_image_visual = pixelate_image(image, pixelation_level = pixelation_level)\n",
    "\n",
    "image_transformed = preprocess_image(image, pixelation_level)\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(image_transformed)\n",
    "    _, predicted = torch.max(outputs, dim = 1)\n",
    "\n",
    "    #softmax takes care of negative outputs\n",
    "    probabilities = F.softmax(outputs, dim = 1) #probabilities[i] that contains softmax for outputs of i+1 image \n",
    "\n",
    "#[(number, probability tensor)...]; enumerate packs into tuple with counter\n",
    "prob_tuples = list(enumerate(probabilities[0])) \n",
    "\n",
    "#to sort by max, extract probability with itemgetter\n",
    "prob_tuples.sort(key = itemgetter(1), reverse = True)\n",
    "\n",
    "for number, probability in prob_tuples:\n",
    "    print(f'Probability of {number}:{probability.item() * 100: .3f}%')\n",
    "\n",
    "print(f'\\nPredicted number: {predicted.item()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __Visualizing the Transformed Image__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.subplot(1,2,1) #create subplot first\n",
    "plt.imshow(image, cmap='gray')\n",
    "plt.axis('off')\n",
    "\n",
    "#dimensions need to be correct\n",
    "plt.subplot(1,2,2)\n",
    "plt.imshow(pixelated_image_visual, cmap='gray')\n",
    "plt.axis('off') \n",
    "\n",
    "plt.show() #not needed for Jupyter"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
